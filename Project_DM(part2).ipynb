{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e17db7f",
   "metadata": {},
   "source": [
    "# Data Mining Project - ABCDEatsInc.\n",
    "\n",
    "**Group 16 members:** <br>\n",
    "- Ana Margarida Valente, 20240936 <br>\n",
    "- Catarina Carneiro, 20240690 <br>\n",
    "- Rui Reis, 20240854 <br>\n",
    "- Mara Mesquita, 20241039 <br>\n",
    "\n",
    "**MSc:** Data Science and Advanced Analytics - Nova IMS <br>\n",
    "**Course:** Data Mining <br>\n",
    "2024/2025\n",
    "\n",
    "## Introduction\n",
    "The client, \"ABCDEatsInc.\", a fictional food delivery service partnering with a range of restaurants to offer diverse meal options, aims to gain a deeper understanding of its customers' behaviors by identifying distinct segments within its database through customer segmentation. A dataset containing data collected over three months from three cities was provided to define, describe, and analyze the resulting clusters. The goal is to uncover actionable insights and briefly recommend marketing strategies tailored to each segment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f64ff41",
   "metadata": {},
   "source": [
    "# **Table of Contents** <br>\n",
    "\n",
    "* [1. Import](#import)\n",
    "     * [1.1 Import libraries](#libraries)<br>\n",
    "     * [1.2 Import the dataset](#dataset)<br>\n",
    " \n",
    "* [2. Feature Selection](#select)<br>\n",
    "\n",
    "* [3. DBSCAN for Outliers](#DBSCAN)<br>\n",
    "  \n",
    "* [4. Clustering](#clustering)<br>\n",
    "    * [4.1 Clustering by perspectives](#perspect)<br>\n",
    "    * [4.2 Cluster Analysis](#analysis)<br>   \n",
    "\n",
    "* [5. Profiling with Categorical features & Unused Features ](#profiling)<br>\n",
    "\n",
    "* [6. Label Outliers](#label)<br>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a17590b",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"import\">\n",
    "\n",
    "## 1. Import \n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8f9eb9",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"libraries\">\n",
    "\n",
    "## 1.1 Import libraries\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5f0d5583",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install minisom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7c5f581a-9c83-4617-8f24-5fbcf514fd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import ceil\n",
    "\n",
    "from itertools import product\n",
    "from scipy.stats import skewnorm\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import joblib\n",
    "from minisom import MiniSom\n",
    "\n",
    "\n",
    "from matplotlib.patches import RegularPolygon, Ellipse ## Import Matplotlib functions to create MiniSOM visualizations\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib import cm, colorbar\n",
    "from matplotlib import colors as mpl_colors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import __version__ as mplver\n",
    "\n",
    "from sklearn.cluster import MeanShift, DBSCAN, estimate_bandwidth\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "\n",
    "# for better resolution plots\n",
    "%config InlineBackend.figure_format = 'retina' # optionally, you can change 'svg' to 'retina'\n",
    "\n",
    "# Setting seaborn style\n",
    "sns.set()\n",
    "\n",
    "# Display all the df and results\n",
    "pd.set_option('display.max_rows', None)  \n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.width', 1000)  \n",
    "pd.set_option('display.colheader_justify', 'center')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f767083",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"dataset\">\n",
    "\n",
    "## 1.2 Import the dataset\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "32603658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv(\"data_beforeclustering.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d12bdd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backup=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8dd55ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.set_index(\"customer_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fd337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4b54d9",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"select\">\n",
    "\n",
    "# 2. Feature Selection\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f24ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6466a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting feature names into groups\n",
    "metric_features = ['customer_age', 'vendor_count', 'product_count','first_order', 'last_order',\n",
    "                   'CUI_American', 'CUI_Asian', 'CUI_Beverages', 'CUI_Cafe','CUI_Chicken Dishes', \n",
    "                   'CUI_Chinese', 'CUI_Desserts', 'CUI_Healthy','CUI_Indian', 'CUI_Italian', \n",
    "                   'CUI_Japanese', 'CUI_Noodle Dishes','CUI_OTHER', 'CUI_Street Food / Snacks', 'CUI_Thai',\n",
    "                   'Sunday','Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday',\n",
    "                   'HR_0', 'HR_1', 'HR_2', 'HR_3', 'HR_4', 'HR_5', 'HR_6', 'HR_7','HR_8', 'HR_9', 'HR_10', \n",
    "                   'HR_11', 'HR_12', 'HR_13', 'HR_14','HR_15', 'HR_16', 'HR_17', 'HR_18', 'HR_19', 'HR_20', 'HR_21','HR_22', 'HR_23', \n",
    "                   'early_morning(0h-5h)', 'morning(6h-11h)','afternoon(12h-17h)', 'night(18h-23h)',\n",
    "                   'Sum_of_Orders', 'recency', 'active_period', 'frequency','total_spend', 'cuisine_diversity',\n",
    "                   'Weekdays','Weekends', \n",
    "                   'Main Courses','Snacks and Street Food', 'Desserts and Beverages','Healthy and Special Diets', 'Other'\n",
    "                   ]\n",
    "\n",
    "non_metric_features = df.columns[df.columns.str.startswith('enc_')].tolist() \n",
    "unused_features = [i for i in df.columns if i not in (metric_features+non_metric_features) ]\n",
    "\n",
    "# Display\n",
    "print('metric_features:', metric_features)\n",
    "print('\\nnon_metric_features:', non_metric_features)\n",
    "print('\\nunused_features:', unused_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b54a96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(24, 18))  # Increased size for clarity\n",
    "\n",
    "# Compute Spearman correlation\n",
    "corr = np.round(df[metric_features].corr(method=\"spearman\"), decimals=2)\n",
    "\n",
    "# Mask for upper triangle and annotations threshold\n",
    "mask_annot = np.absolute(corr.values) >= 0.5\n",
    "annot = np.where(mask_annot, corr.values, np.full(corr.shape, \"\"))  # Annotate only significant correlations\n",
    "\n",
    "# Mask upper triangle\n",
    "matrix = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Customize the heatmap\n",
    "sns.heatmap(\n",
    "    data=corr,\n",
    "    annot=annot,                   # Show annotations\n",
    "    mask=matrix,                   # Apply mask to hide upper triangle\n",
    "    cmap='PiYG',               # Adjust colormap for better clarity\n",
    "    fmt='',                        # Avoid formatting issues\n",
    "    annot_kws={\"size\": 8},        # Font size for annotations\n",
    "    vmin=-1, vmax=1, center=0,     # Set limits for the colormap\n",
    "    square=True, linewidths=.5,    # Keep square cells with a border\n",
    "    cbar_kws={\"shrink\": 0.8}       # Shrink color bar for better fit\n",
    ")\n",
    "\n",
    "# Title and labels\n",
    "fig.subplots_adjust(top=0.9)       # Adjust layout to fit the title\n",
    "fig.suptitle(\"Correlation Matrix\", fontsize=22, weight='bold')  # Bold, larger title\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)  # Rotate x-axis labels\n",
    "plt.yticks(rotation=0, fontsize=12)               # Keep y-axis labels horizontal\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e858c6",
   "metadata": {},
   "source": [
    "Drop from dataset HR, CUI and DOW variables since we have them explicit in other features\n",
    "\n",
    "- HR - Time periods\n",
    "- CUI - Types of cuisine\n",
    "- DOW - Week days and Weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2c96af80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = ['HR_0', 'HR_1', 'HR_2', 'HR_3', 'HR_4', 'HR_5', 'HR_6', 'HR_7', 'HR_8', 'HR_9', \n",
    "                   'HR_10', 'HR_11', 'HR_12', 'HR_13', 'HR_14', 'HR_15', 'HR_16', 'HR_17', 'HR_18', \n",
    "                   'HR_19', 'HR_20', 'HR_21', 'HR_22', 'HR_23',\n",
    "                   'Sunday','Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday',\n",
    "                   'CUI_American', 'CUI_Asian', 'CUI_Beverages', 'CUI_Cafe','CUI_Chicken Dishes', \n",
    "                   'CUI_Chinese', 'CUI_Desserts', 'CUI_Healthy','CUI_Indian', 'CUI_Italian', \n",
    "                   'CUI_Japanese', 'CUI_Noodle Dishes','CUI_OTHER', 'CUI_Street Food / Snacks', 'CUI_Thai']\n",
    "\n",
    "metric_features = [i for i in metric_features if i not in columns_to_drop]\n",
    "\n",
    "df=df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f8b5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 14))  # Increased size for clarity\n",
    "\n",
    "# Compute Spearman correlation\n",
    "corr = np.round(df[metric_features].corr(method=\"spearman\"), decimals=2)\n",
    "\n",
    "# Mask for upper triangle and annotations threshold\n",
    "mask_annot = np.absolute(corr.values) >= 0.5\n",
    "annot = np.where(mask_annot, corr.values, np.full(corr.shape, \"\"))  # Annotate only significant correlations\n",
    "\n",
    "# Mask upper triangle\n",
    "matrix = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Customize the heatmap\n",
    "sns.heatmap(\n",
    "    data=corr,\n",
    "    annot=annot,                   # Show annotations\n",
    "    mask=matrix,                   # Apply mask to hide upper triangle\n",
    "    cmap='PiYG',               # Adjust colormap for better clarity\n",
    "    fmt='',                        # Avoid formatting issues\n",
    "    annot_kws={\"size\": 8},        # Font size for annotations\n",
    "    vmin=-1, vmax=1, center=0,     # Set limits for the colormap\n",
    "    square=True, linewidths=.5,    # Keep square cells with a border\n",
    "    cbar_kws={\"shrink\": 0.8}       # Shrink color bar for better fit\n",
    ")\n",
    "\n",
    "# Title and labels\n",
    "fig.subplots_adjust(top=0.9)       # Adjust layout to fit the title\n",
    "fig.suptitle(\"Correlation Matrix\", fontsize=22, weight='bold')  # Bold, larger title\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)  # Rotate x-axis labels\n",
    "plt.yticks(rotation=0, fontsize=12)               # Keep y-axis labels horizontal\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6e859e",
   "metadata": {},
   "source": [
    "## SOM - Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85fdf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = N = 64\n",
    "neighborhood_function = \"gaussian\"\n",
    "topology = \"hexagonal\"\n",
    "n_feats = len(metric_features)\n",
    "learning_rate = 0.7\n",
    "\n",
    "\n",
    "som_data = df[metric_features].values\n",
    "\n",
    "sm = MiniSom(M, N,              # 10x10 map size\n",
    "             n_feats,           # Number of the elements of the vectors in input.\n",
    "             learning_rate=learning_rate, \n",
    "             topology=topology, \n",
    "             neighborhood_function=neighborhood_function, \n",
    "             activation_distance='euclidean',\n",
    "             random_seed=42\n",
    "             )\n",
    "\n",
    "# Initializes the weights of the SOM picking random samples from data.\n",
    "sm.random_weights_init(som_data) \n",
    "\n",
    "\n",
    "print(\"Before training:\")\n",
    "print(\"QE\", np.round(sm.quantization_error(som_data),4))\n",
    "print(\"TE\", np.round(sm.topographic_error(som_data),4))\n",
    "\n",
    "\n",
    "\n",
    "# Trains the SOM using all the vectors in data sequentially\n",
    "# minisom does not distinguish between unfolding and fine tuning phase;\n",
    "\n",
    "sm.train_batch(som_data, 20000)\n",
    "\n",
    "print(\"After training:\")\n",
    "print(\"QE\", np.round(sm.quantization_error(som_data),4))\n",
    "print(\"TE\", np.round(sm.topographic_error(som_data),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5735a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = sm.get_weights()\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a1569203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hexagons(som,              # Trained SOM model \n",
    "                  sf,               # matplotlib figure object\n",
    "                  colornorm,        # colornorm\n",
    "                  matrix_vals,      # SOM weights or\n",
    "                  label=\"\",         # title for figure\n",
    "                  cmap=cm.Grays,    # colormap to use\n",
    "                  annot=False       \n",
    "                  ):\n",
    "\n",
    "    \n",
    "    axs = sf.subplots(1,1)\n",
    "    \n",
    "    for i in range(matrix_vals.shape[0]):\n",
    "        for j in range(matrix_vals.shape[1]):\n",
    "\n",
    "            wx, wy = som.convert_map_to_euclidean((i,j)) \n",
    "\n",
    "            hex = RegularPolygon((wx, wy), \n",
    "                                numVertices=6, \n",
    "                                radius= np.sqrt(1/3),\n",
    "                                facecolor=cmap(colornorm(matrix_vals[i, j])), \n",
    "                                alpha=1, \n",
    "                                edgecolor='white',\n",
    "                                linewidth=.5)\n",
    "            axs.add_patch(hex)\n",
    "            if annot==True:\n",
    "                annot_val = np.round(matrix_vals[i,j],2)\n",
    "                if int(annot_val) == annot_val:\n",
    "                    annot_val = int(annot_val)\n",
    "                axs.text(wx,wy, annot_val, \n",
    "                        ha='center', va='center', \n",
    "                        fontsize='x-small')\n",
    "\n",
    "\n",
    "    ## Remove axes for hex plot\n",
    "    axs.margins(.05)\n",
    "    axs.set_aspect('equal')\n",
    "    axs.axis(\"off\")\n",
    "    axs.set_title(label)\n",
    "\n",
    "    \n",
    "\n",
    "    # ## Add colorbar\n",
    "    divider = make_axes_locatable(axs)\n",
    "    ax_cb = divider.append_axes(\"right\", size=\"5%\", pad=\"0%\")\n",
    "\n",
    "    ## Create a Mappable object\n",
    "    cmap_sm = plt.cm.ScalarMappable(cmap=cmap, norm=colornorm)\n",
    "    cmap_sm.set_array([])\n",
    "\n",
    "    ## Create custom colorbar \n",
    "    cb1 = colorbar.Colorbar(ax_cb,\n",
    "                            orientation='vertical', \n",
    "                            alpha=1,\n",
    "                            mappable=cmap_sm\n",
    "                            )\n",
    "    cb1.ax.get_yaxis().labelpad = 6\n",
    "\n",
    "    # Add colorbar to plot\n",
    "    sf.add_axes(ax_cb)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return sf \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137014ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Component Planes\n",
    "figsize=(20,17)\n",
    "fig = plt.figure(figsize=figsize, constrained_layout=True, dpi=128, )\n",
    "\n",
    "subfigs = fig.subfigures(5,5,wspace=.15)\n",
    "\n",
    "colornorm = mpl_colors.Normalize(vmin=np.min(weights), vmax=np.max(weights))\n",
    "\n",
    "for cpi, sf in zip(range(len(metric_features)), subfigs.flatten()):\n",
    "    \n",
    "    matrix_vals = weights[:,:,cpi]\n",
    "    vext = np.max(np.abs([np.min(matrix_vals), np.max(matrix_vals)]))\n",
    "    colornorm = mpl_colors.Normalize(vmin=np.min(matrix_vals), vmax=np.max(matrix_vals))\n",
    "    # colornorm = mpl_colors.CenteredNorm(vcenter=0, halfrange=vext)\n",
    "\n",
    "\n",
    "    sf = plot_hexagons(sm, sf, \n",
    "                    colornorm,\n",
    "                    matrix_vals,\n",
    "                    label=metric_features[cpi],\n",
    "                    cmap=cm.coolwarm,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8fd7838f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select variables according to their correlations\n",
    "# Updating metric_features\n",
    "metric_features.remove('vendor_count') #cuisine_diversity (0.90)\n",
    "metric_features.remove('product_count') #Sum_of_orders (0.95)\n",
    "metric_features.remove('last_order') #recency (-1.0)\n",
    "\n",
    "\n",
    "#Time Periods\n",
    "metric_features.remove('early_morning(0h-5h)')\n",
    "metric_features.remove('morning(6h-11h)')\n",
    "metric_features.remove('afternoon(12h-17h)')\n",
    "metric_features.remove('night(18h-23h)')\n",
    "\n",
    "#Type of Meal\n",
    "metric_features.remove('Main Courses')\n",
    "metric_features.remove('Snacks and Street Food')\n",
    "metric_features.remove('Desserts and Beverages')\n",
    "metric_features.remove('Healthy and Special Diets')\n",
    "metric_features.remove('Other')\n",
    "\n",
    "\n",
    "unused_features.extend(['vendor_count','product_count','last_order',\n",
    "                      'early_morning(0h-5h)','morning(6h-11h)','afternoon(12h-17h)','night(18h-23h)',\n",
    "                       'Main Courses','Snacks and Street Food','Desserts and Beverages','Healthy and Special Diets','Other'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb282b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(24, 18))  # Increased size for clarity\n",
    "\n",
    "# Compute Spearman correlation\n",
    "corr = np.round(df[metric_features].corr(method=\"spearman\"), decimals=2)\n",
    "\n",
    "# Mask for annotations near 0 (e.g., absolute correlation < 0.2)\n",
    "mask_annot = (np.absolute(corr.values) < 0.2)  # You can adjust the threshold to be closer to 0\n",
    "annot = np.where(mask_annot, corr.values, np.full(corr.shape, \"\"))  # Annotate only near-zero correlations\n",
    "\n",
    "# Mask upper triangle\n",
    "matrix = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Customize the heatmap\n",
    "sns.heatmap(\n",
    "    data=corr,\n",
    "    annot=annot,                   # Show annotations for near-zero correlations\n",
    "    mask=matrix,                   # Apply mask to hide upper triangle\n",
    "    cmap='PiYG',                   # Adjust colormap for better clarity\n",
    "    fmt='',                        # Avoid formatting issues\n",
    "    annot_kws={\"size\": 8},         # Font size for annotations\n",
    "    vmin=-1, vmax=1, center=0,     # Set limits for the colormap\n",
    "    square=True, linewidths=.5,    # Keep square cells with a border\n",
    "    cbar_kws={\"shrink\": 0.8}       # Shrink color bar for better fit\n",
    ")\n",
    "\n",
    "# Title and labels\n",
    "fig.subplots_adjust(top=0.9)       # Adjust layout to fit the title\n",
    "fig.suptitle(\"Correlation Matrix (Near Zero)\", fontsize=22, weight='bold')  # Bold, larger title\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)  # Rotate x-axis labels\n",
    "plt.yticks(rotation=0, fontsize=12)               # Keep y-axis labels horizontal\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3478a057",
   "metadata": {},
   "source": [
    "**Customer_age** does not seem to have a relevant correlation to any variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4a572c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_features.remove('customer_age')\n",
    "unused_features.extend(['customer_age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58211f1c",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"DBSCAN\">\n",
    "\n",
    "# 3. DBSCAN for Outliers\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining eps and min_samples:\n",
    "- **MinPts**: As a rule of thumb, **minPts = 2 x dim** can be used.\n",
    "\n",
    "- **ε**: The value for ε can then be chosen by using a **k-distance graph**, plotting the distance to the kth (k = minPts - 1) nearest neighbor ordered from the largest to the smallest value. Good values of ε are where this plot shows an **\"elbow\"**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13dc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding dim\n",
    "dim=len(metric_features)\n",
    "print(f'dim = {dim}')\n",
    "print(f'MinPts = {2*dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd73649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute k-distances\n",
    "n_neighbors = (dim*2)-1  # Number of neighbors to consider (MinPts-1)\n",
    "neigh = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "neigh.fit(df[metric_features])\n",
    "distances, _ = neigh.kneighbors(df[metric_features])\n",
    "\n",
    "# Sort distances for plotting\n",
    "distances = np.sort(distances[:, -1])\n",
    "\n",
    "# Plot k-distance graph\n",
    "plt.figure(figsize=(8, 6)) \n",
    "plt.plot(distances, color='steelblue', linewidth=2, label=f\"{n_neighbors}th Nearest Neighbor Distance\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"K-Distance Graph to Determine Optimal Epsilon\", fontsize=16, pad=15)\n",
    "plt.xlabel(\"Points (Sorted by Distance)\", fontsize=14, labelpad=10)\n",
    "plt.ylabel(f\"Distance to {n_neighbors}th Nearest Neighbor\", fontsize=14, labelpad=10)\n",
    "\n",
    "# Add a grid\n",
    "plt.grid(visible=True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# Highlight the \"elbow\" area\n",
    "plt.axhline(y=1.5, color='red', linestyle=\"--\", linewidth=1.5, label=\"Potential Elbow Region\")\n",
    "plt.axhline(y=0.5, color='red', linestyle=\"--\", linewidth=1.5)\n",
    "plt.axhline(y=1, color='green', linestyle=\"--\", linewidth=1.5)\n",
    "\n",
    "# Customize ticks\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(fontsize=12, loc=\"upper left\")\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292b604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=1, min_samples=dim*2, n_jobs=-1)\n",
    "dbscan_labels = dbscan.fit_predict(df[metric_features])\n",
    "\n",
    "dbscan_n_clusters = len(np.unique(dbscan_labels))\n",
    "print(\"Number of estimated clusters : %d\" % dbscan_n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d859e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(dbscan_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65d6112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the labels to df\n",
    "df_concat = pd.concat([df[metric_features], pd.Series(dbscan_labels, index=df.index, name=\"dbscan_labels\")], axis=1)\n",
    "df_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33031629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting noise (potential outliers)\n",
    "num_noise = len(df_concat.loc[df_concat['dbscan_labels'] == -1])\n",
    "total_points = len(df_concat)\n",
    "percent_outliers = (num_noise / total_points) * 100\n",
    "\n",
    "print(num_noise)\n",
    "print(f\"Percentage of potencial outliers: {percent_outliers:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2589f57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ss(df):\n",
    "    \"\"\"Computes the sum of squares for all variables given a dataset\n",
    "    \"\"\"\n",
    "    ss = np.sum(df.var() * (df.count() - 1))\n",
    "    return ss  # return sum of sum of squares of each df variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13960401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the R^2 of the cluster solution\n",
    "df_nonoise = df_concat.loc[df_concat['dbscan_labels'] != -1]\n",
    "sst = get_ss(df[metric_features])  # get total sum of squares\n",
    "ssw_labels = df_nonoise.groupby(by='dbscan_labels').apply(get_ss)  # compute ssw for each cluster labels\n",
    "ssb = sst - np.sum(ssw_labels)  # SST = SSW + SSB\n",
    "r2 = ssb / sst\n",
    "print(\"Cluster solution with R^2 of %0.4f\" % r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c96ab586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the newly detected outliers (they will be classified later based on the final clusters)\n",
    "df_out = df[dbscan_labels==-1].copy()\n",
    "\n",
    "# New df without outliers \n",
    "df = df[dbscan_labels!=-1].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dae9c6d",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"clustering\">\n",
    "\n",
    "# 4. Clustering\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8d762f",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"perspect\">\n",
    "\n",
    "## 4.1 Clustering by perspectives\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ac0fd789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Value-Based Segmentation\n",
    "value_based_features = ['total_spend', 'frequency', 'recency','active_period']\n",
    "\n",
    "# 2. Behaviour-Based Segmentation\n",
    "behaviour_based_features = ['first_order','cuisine_diversity','Weekdays', 'Weekends','Sum_of_Orders']\n",
    "\n",
    "\n",
    "df_value = df[value_based_features].copy()\n",
    "df_beh = df[behaviour_based_features].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ec4149b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the clusterers\n",
    "kmeans = KMeans(\n",
    "    init='k-means++',\n",
    "    n_init=40,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "hierarchical = AgglomerativeClustering(\n",
    "    metric='euclidean'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e4042eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ss(df, feats):\n",
    "    \"\"\"\n",
    "    Calculate the sum of squares (SS) for the given DataFrame.\n",
    "\n",
    "    The sum of squares is computed as the sum of the variances of each column\n",
    "    multiplied by the number of non-NA/null observations minus one.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame for which the sum of squares is to be calculated.\n",
    "    feats (list of str): A list of feature column names to be used in the calculation.\n",
    "\n",
    "    Returns:\n",
    "    float: The sum of squares of the DataFrame.\n",
    "    \"\"\"\n",
    "    df_ = df[feats]\n",
    "    ss = np.sum(df_.var() * (df_.count() - 1))\n",
    "    \n",
    "    return ss \n",
    "\n",
    "\n",
    "def get_ssb(df, feats, label_col):\n",
    "    \"\"\"\n",
    "    Calculate the between-group sum of squares (SSB) for the given DataFrame.\n",
    "    The between-group sum of squares is computed as the sum of the squared differences\n",
    "    between the mean of each group and the overall mean, weighted by the number of observations\n",
    "    in each group.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame containing the data.\n",
    "    feats (list of str): A list of feature column names to be used in the calculation.\n",
    "    label_col (str): The name of the column in the DataFrame that contains the group labels.\n",
    "    \n",
    "    Returns\n",
    "    float: The between-group sum of squares of the DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    ssb_i = 0\n",
    "    for i in np.unique(df[label_col]):\n",
    "        df_ = df.loc[:, feats]\n",
    "        X_ = df_.values\n",
    "        X_k = df_.loc[df[label_col] == i].values\n",
    "        \n",
    "        ssb_i += (X_k.shape[0] * (np.square(X_k.mean(axis=0) - X_.mean(axis=0))) )\n",
    "\n",
    "    ssb = np.sum(ssb_i)\n",
    "    \n",
    "\n",
    "    return ssb\n",
    "\n",
    "\n",
    "def get_ssw(df, feats, label_col):\n",
    "    \"\"\"\n",
    "    Calculate the sum of squared within-cluster distances (SSW) for a given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame containing the data.\n",
    "    feats (list of str): A list of feature column names to be used in the calculation.\n",
    "    label_col (str): The name of the column containing cluster labels.\n",
    "\n",
    "    Returns:\n",
    "    float: The sum of squared within-cluster distances (SSW).\n",
    "    \"\"\"\n",
    "    feats_label = feats+[label_col]\n",
    "\n",
    "    df_k = df[feats_label].groupby(by=label_col).apply(lambda col: get_ss(col, feats), \n",
    "                                                       include_groups=False)\n",
    "\n",
    "    return df_k.sum()\n",
    "\n",
    "def get_rsq(df, feats, label_col):\n",
    "    \"\"\"\n",
    "    Calculate the R-squared value for a given DataFrame and features.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing the data.\n",
    "    feats (list): A list of feature column names to be used in the calculation.\n",
    "    label_col (str): The name of the column containing the labels or cluster assignments.\n",
    "\n",
    "    Returns:\n",
    "    float: The R-squared value, representing the proportion of variance explained by the clustering.\n",
    "    \"\"\"\n",
    "\n",
    "    df_sst_ = get_ss(df, feats)                 # get total sum of squares\n",
    "    df_ssw_ = get_ssw(df, feats, label_col)     # get ss within\n",
    "    df_ssb_ = df_sst_ - df_ssw_                 # get ss between\n",
    "\n",
    "    # r2 = ssb/sst \n",
    "    return (df_ssb_/df_sst_)\n",
    "\n",
    "def get_r2_scores(df, feats, clusterer, min_k=1, max_k=10):\n",
    "    \"\"\"\n",
    "    Loop over different values of k. To be used with sklearn clusterers.\n",
    "    \"\"\"\n",
    "    r2_clust = {}\n",
    "    for n in range(min_k, max_k):\n",
    "        clust = clone(clusterer).set_params(n_clusters=n)\n",
    "        labels = clust.fit_predict(df)\n",
    "        df_concat = pd.concat([df, \n",
    "                               pd.Series(labels, name='labels', index=df.index)], axis=1)  \n",
    "\n",
    "        r2_clust[n] = get_rsq(df_concat, feats, 'labels' )\n",
    "    return r2_clust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bf7c76",
   "metadata": {},
   "source": [
    "### 1. Value-Based "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26601c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_clusters = range(1, 10)\n",
    "inertia = []\n",
    "for n_clus in range_clusters:  # iterate over desired ncluster range\n",
    "    kmclust = KMeans(n_clusters=n_clus, init='k-means++', n_init=40, random_state=1)\n",
    "    kmclust.fit(df_value)\n",
    "    inertia.append(kmclust.inertia_)  # save the inertia of the given cluster solution\n",
    "\n",
    "# The inertia plot\n",
    "fig, ax = plt.subplots(figsize=(9,5))\n",
    "\n",
    "ax.plot(range_clusters, inertia)\n",
    "ax.set_xticks(range_clusters)\n",
    "ax.set_ylabel(\"Inertia: SSw\")\n",
    "ax.set_xlabel(\"Number of clusters\")\n",
    "ax.set_title(\"Inertia plot over clusters\", size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08bef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing average silhouette metric\n",
    "avg_silhouette = []\n",
    "for nclus in range_clusters:\n",
    "    #Skip nclus == 1\n",
    "    if nclus == 1:\n",
    "        continue\n",
    "    \n",
    "    #Create a figure\n",
    "    fig = plt.figure(figsize=(13, 7))\n",
    "\n",
    "    #Initialize the KMeans object with n_clusters value and a random generator\n",
    "    #seed of 10 for reproducibility.\n",
    "    kmclust = KMeans(n_clusters=nclus, init='k-means++', n_init=40, random_state=1)\n",
    "    cluster_labels = kmclust.fit_predict(df_value)\n",
    "\n",
    "    #The silhouette_score gives the average value for all the samples.\n",
    "    #This gives a perspective into the density and separation of the formed clusters\n",
    "    silhouette_avg = silhouette_score(df_value, cluster_labels)\n",
    "    avg_silhouette.append(silhouette_avg)\n",
    "    print(f\"For n_clusters = {nclus}, the average silhouette_score is : {silhouette_avg}\")\n",
    "\n",
    "    #Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(df_value, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(nclus):\n",
    "        #Aggregate the silhouette scores for samples belonging to cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        \n",
    "        #Get y_upper to demarcate silhouette y range size\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        \n",
    "        #Filling the silhouette\n",
    "        color = cm.nipy_spectral(float(i) / nclus)\n",
    "        plt.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        #Label the silhouette plots with their cluster numbers at the middle\n",
    "        plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        #Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    plt.title(\"The silhouette plot for the various clusters.\")\n",
    "    plt.xlabel(\"The silhouette coefficient values\")\n",
    "    plt.ylabel(\"Cluster label\")\n",
    "\n",
    "    #The vertical line for average silhouette score of all the values\n",
    "    plt.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    \n",
    "    #The silhouette coefficient can range from -1, 1\n",
    "    xmin, xmax = np.round(sample_silhouette_values.min() -0.1, 2), np.round(sample_silhouette_values.max() + 0.1, 2)\n",
    "    plt.xlim([xmin, xmax])\n",
    "    \n",
    "    #The (nclus+1)*10 is for inserting blank space between silhouette\n",
    "    #plots of individual clusters, to demarcate them clearly.\n",
    "    plt.ylim([0, len(df_value) + (nclus + 1) * 10])\n",
    "\n",
    "    plt.yticks([])  # Clear the yaxis labels / ticks\n",
    "    plt.xticks(np.arange(xmin, xmax, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "68590047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining the R² scores for each cluster solution on value based variables\n",
    "r2_scores = {}\n",
    "\n",
    "r2_scores['kmeans'] = get_r2_scores(df_value, value_based_features, kmeans)\n",
    "\n",
    "for linkage in ['complete', 'average', 'single', 'ward']:\n",
    "    r2_scores[linkage] = get_r2_scores(\n",
    "        df_value,                 # data\n",
    "        value_based_features,   # features of perspective\n",
    "        # use HClust, changing the linkage at each iteration\n",
    "        hierarchical.set_params(linkage=linkage) \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f614da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_scores_df = pd.DataFrame(r2_scores)\n",
    "r2_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac0ebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the R² scores for each cluster solution on value based variables\n",
    "r2_scores_df.plot.line(figsize=(10,7))\n",
    "\n",
    "plt.title(\"Value Based Variables:\\nR² plot for various clustering methods\\n\", fontsize=21)\n",
    "plt.legend(title=\"Cluster methods\", title_fontsize=11)\n",
    "plt.xlabel(\"Number of clusters\", fontsize=13)\n",
    "plt.ylabel(\"R² metric\", fontsize=13) \n",
    "plt.show()      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd905ad",
   "metadata": {},
   "source": [
    "### 2. Behaviour-Based "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab4419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_clusters = range(1, 10)\n",
    "inertia = []\n",
    "for n_clus in range_clusters:  # iterate over desired ncluster range\n",
    "    kmclust = KMeans(n_clusters=n_clus, init='k-means++', n_init=40, random_state=1)\n",
    "    kmclust.fit(df_beh)\n",
    "    inertia.append(kmclust.inertia_)  # save the inertia of the given cluster solution\n",
    "\n",
    "# The inertia plot\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,5))\n",
    "\n",
    "ax.plot(range_clusters, inertia)\n",
    "ax.set_xticks(range_clusters)\n",
    "ax.set_ylabel(\"Inertia: SSw\")\n",
    "ax.set_xlabel(\"Number of clusters\")\n",
    "ax.set_title(\"Inertia plot over clusters\", size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f63a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing average silhouette metric\n",
    "avg_silhouette = []\n",
    "for nclus in range_clusters:\n",
    "    #Skip nclus == 1\n",
    "    if nclus == 1:\n",
    "        continue\n",
    "    \n",
    "    #Create a figure\n",
    "    fig = plt.figure(figsize=(13, 7))\n",
    "\n",
    "    #Initialize the KMeans object with n_clusters value and a random generator\n",
    "    #seed of 10 for reproducibility.\n",
    "    kmclust = KMeans(n_clusters=nclus, init='k-means++', n_init=40, random_state=1)\n",
    "    cluster_labels = kmclust.fit_predict(df_value)\n",
    "\n",
    "    #The silhouette_score gives the average value for all the samples.\n",
    "    #This gives a perspective into the density and separation of the formed clusters\n",
    "    silhouette_avg = silhouette_score(df_value, cluster_labels)\n",
    "    avg_silhouette.append(silhouette_avg)\n",
    "    print(f\"For n_clusters = {nclus}, the average silhouette_score is : {silhouette_avg}\")\n",
    "\n",
    "    #Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(df_value, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(nclus):\n",
    "        #Aggregate the silhouette scores for samples belonging to cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        \n",
    "        #Get y_upper to demarcate silhouette y range size\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        \n",
    "        #Filling the silhouette\n",
    "        color = cm.nipy_spectral(float(i) / nclus)\n",
    "        plt.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        #Label the silhouette plots with their cluster numbers at the middle\n",
    "        plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        #Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    plt.title(\"The silhouette plot for the various clusters.\")\n",
    "    plt.xlabel(\"The silhouette coefficient values\")\n",
    "    plt.ylabel(\"Cluster label\")\n",
    "\n",
    "    #The vertical line for average silhouette score of all the values\n",
    "    plt.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    \n",
    "    #The silhouette coefficient can range from -1, 1\n",
    "    xmin, xmax = np.round(sample_silhouette_values.min() -0.1, 2), np.round(sample_silhouette_values.max() + 0.1, 2)\n",
    "    plt.xlim([xmin, xmax])\n",
    "    \n",
    "    #The (nclus+1)*10 is for inserting blank space between silhouette\n",
    "    #plots of individual clusters, to demarcate them clearly.\n",
    "    plt.ylim([0, len(df_value) + (nclus + 1) * 10])\n",
    "\n",
    "    plt.yticks([])  # Clear the yaxis labels / ticks\n",
    "    plt.xticks(np.arange(xmin, xmax, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b1e96c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the R² scores for each cluster solution on Behaviour based variables\n",
    "r2_scores_beh = {}\n",
    "\n",
    "r2_scores_beh['kmeans'] = get_r2_scores(df_beh,behaviour_based_features, kmeans)\n",
    "\n",
    "for linkage in ['complete', 'average', 'single', 'ward']:\n",
    "    r2_scores_beh[linkage] = get_r2_scores(\n",
    "        df_beh,                 # data\n",
    "        behaviour_based_features,   # features of perspective\n",
    "        # use HClust, changing the linkage at each iteration\n",
    "        hierarchical.set_params(linkage=linkage) \n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a3a308",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_scores_df_beh = pd.DataFrame(r2_scores_beh)\n",
    "r2_scores_df_beh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607c4d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the R² scores for each cluster solution on Behaviour based variables\n",
    "r2_scores_df_beh.plot.line(figsize=(10,7))\n",
    "\n",
    "plt.title(\"Preference Based:\\nR² plot for various clustering methods\\n\", fontsize=21)\n",
    "plt.legend(title=\"Cluster methods\", title_fontsize=11)\n",
    "plt.xlabel(\"Number of clusters\", fontsize=13)\n",
    "plt.ylabel(\"R² metric\", fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3227827",
   "metadata": {},
   "source": [
    "### 3. Merging the Perspectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1cd3834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the right clustering (algorithm and number of clusters) for each perspective\n",
    "kmeans_value = KMeans(\n",
    "    n_clusters=2,\n",
    "    init='k-means++',\n",
    "    n_init=40,\n",
    "    random_state=42\n",
    ")\n",
    "value_labels = kmeans_value.fit_predict(df_value)\n",
    "\n",
    "kmeans_beh = KMeans(\n",
    "    n_clusters=3,\n",
    "    init='k-means++',\n",
    "    n_init=40,\n",
    "    random_state=42\n",
    ")\n",
    "behaviour_labels = kmeans_beh.fit_predict(df_beh)\n",
    "\n",
    "\n",
    "df['value_labels'] = value_labels\n",
    "df['behaviour_labels'] = behaviour_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2967777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette score for value-based clustering\n",
    "silhouette_value = silhouette_score(df_value, value_labels)\n",
    "print(f\"Silhouette Score (Value Clustering): {silhouette_value:.2f}\")\n",
    "\n",
    "# Silhouette score for behavior-based clustering\n",
    "silhouette_beh = silhouette_score(df_beh, behaviour_labels)\n",
    "print(f\"Silhouette Score (Behavior Clustering): {silhouette_beh:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4abed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count label frequencies (contigency table)\n",
    "\n",
    "pd.crosstab(df['value_labels'],\n",
    "            df['behaviour_labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21da2af",
   "metadata": {},
   "source": [
    "### 3.1 Manual merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c392f83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get centroids of clusters based on value, preference, and behavior labels\n",
    "df_centroids = df.groupby(['value_labels', 'behaviour_labels'])[metric_features].mean()\n",
    "\n",
    "df_centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "00dbebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clusters with low frequency to be merged (<5000):\n",
    "# (Value_label, Behaviour_label)\n",
    "to_merge = [(0,1),(0,2),(1,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e195de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the euclidean distance matrix between the centroids\n",
    "centroid_dists = euclidean = pairwise_distances(df_centroids)\n",
    "\n",
    "df_dists = pd.DataFrame(\n",
    "    centroid_dists, \n",
    "    columns=df_centroids.index, \n",
    "    index=df_centroids.index\n",
    ")\n",
    "\n",
    "df_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a352ee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging each low frequency clustering (source) \n",
    "# to the closest cluster (target)\n",
    "\n",
    "source_target = {}\n",
    "\n",
    "for clus in to_merge:\n",
    "    # If cluster to merge (source) has not yet been used as target\n",
    "    if clus not in source_target.values():\n",
    "        # Add this cluster to source_target map as key\n",
    "        # Use the cluster with the smallest distance to it as value\n",
    "        source_target[clus] = df_dists.loc[clus].sort_values().index[1]\n",
    "\n",
    "source_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c7ab40ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = df.copy()\n",
    "\n",
    "# Changing the Value_labels and Behaviour_labels based on source_target\n",
    "for source, target in source_target.items():\n",
    "    mask = (df_manual['value_labels']==source[0]) & (df_manual['behaviour_labels']==source[1])\n",
    "    df_manual.loc[mask, 'value_labels'] = target[0]\n",
    "    df_manual.loc[mask, 'behaviour_labels'] = target[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43011c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New contigency table\n",
    "\n",
    "pd.crosstab(df_manual['value_labels'],\n",
    "            df_manual['behaviour_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a7c866f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column combining 'value_labels' and 'behaviour_labels'\n",
    "df_manual['merged_labels_'] = df_manual['value_labels'].astype(str) + \"_\" + df_manual['behaviour_labels'].astype(str)\n",
    "\n",
    "# Use factorize to assign a unique number to each unique combination in 'merged_labels'\n",
    "df_manual['merged_labels'] = pd.factorize(df_manual['merged_labels_'])[0]\n",
    "\n",
    "# Drop the string 'merged_labels' column if not needed\n",
    "df_manual = df_manual.drop(columns=['merged_labels_'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b642a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged cluster centroids\n",
    "df_manual.groupby('merged_labels').mean(numeric_only=True)[metric_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8432e984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a frequency table for 'merged_labels'\n",
    "frequency_table = df_manual['merged_labels'].value_counts().reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "frequency_table.columns = ['Merged Label', 'Frequency']\n",
    "\n",
    "# Sort by merged labels for better readability (optional)\n",
    "frequency_table = frequency_table.sort_values(by='Merged Label').reset_index(drop=True)\n",
    "\n",
    "# Display the table\n",
    "frequency_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964ea348",
   "metadata": {},
   "source": [
    "### 3.2 Merging using Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d03f2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centroids of the concatenated cluster labels\n",
    "df_centroids = df.groupby(['value_labels', 'behaviour_labels'])\\\n",
    "    [metric_features].mean()\n",
    "df_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4a08d318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Hierarchical clustering to merge the concatenated cluster centroids\n",
    "linkage = 'ward'\n",
    "hclust = AgglomerativeClustering(\n",
    "    linkage=linkage, \n",
    "    metric='euclidean', \n",
    "    distance_threshold=0, \n",
    "    n_clusters=None\n",
    ")\n",
    "\n",
    "hclust_labels = hclust.fit_predict(df_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cfd675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py\n",
    "\n",
    "# create the counts of samples under each node (number of points being merged)\n",
    "counts = np.zeros(hclust.children_.shape[0])\n",
    "n_samples = len(hclust.labels_)\n",
    "\n",
    "# hclust.children_ contains the observation ids that are being merged together\n",
    "# At the i-th iteration, children[i][0] and children[i][1] are merged to form node n_samples + i\n",
    "for i, merge in enumerate(hclust.children_):\n",
    "    # track the number of observations in the current cluster being formed\n",
    "    current_count = 0\n",
    "    for child_idx in merge:\n",
    "        if child_idx < n_samples:\n",
    "            # If this is True, then we are merging an observation\n",
    "            current_count += 1  # leaf node\n",
    "        else:\n",
    "            # Otherwise, we are merging a previously formed cluster\n",
    "            current_count += counts[child_idx - n_samples]\n",
    "    counts[i] = current_count\n",
    "\n",
    "# the hclust.children_ is used to indicate the two points/clusters being merged (dendrogram's u-joins)\n",
    "# the hclust.distances_ indicates the distance between the two points/clusters (height of the u-joins)\n",
    "# the counts indicate the number of points being merged (dendrogram's x-axis)\n",
    "linkage_matrix = np.column_stack(\n",
    "    [hclust.children_, hclust.distances_, counts]\n",
    ").astype(float)\n",
    "\n",
    "# Plot the corresponding dendrogram\n",
    "sns.set()\n",
    "fig = plt.figure(figsize=(11,5))\n",
    "# The Dendrogram parameters need to be tuned\n",
    "\n",
    "y_threshold = 3.5\n",
    "\n",
    "dendrogram(linkage_matrix, \n",
    "           truncate_mode='level', \n",
    "           labels=df_centroids.index, p=5, \n",
    "           color_threshold=y_threshold, \n",
    "           above_threshold_color='k')\n",
    "\n",
    "plt.hlines(y_threshold, 0, 1000, colors=\"r\", linestyles=\"dashed\")\n",
    "plt.title(f'Hierarchical Clustering - {linkage.title()}\\'s Dendrogram', fontsize=21)\n",
    "plt.xlabel('Number of points in node (or index of point if no parenthesis)')\n",
    "plt.ylabel(f'Euclidean Distance', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6096c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-running the Hierarchical clustering based on the correct number of clusters\n",
    "hclust = AgglomerativeClustering(\n",
    "    linkage='ward', \n",
    "    metric='euclidean', \n",
    "    n_clusters=3\n",
    ")\n",
    "hclust_labels = hclust.fit_predict(df_centroids)\n",
    "df_centroids['hclust_labels'] = hclust_labels\n",
    "\n",
    "df_centroids  # centroid's cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097a0410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapper between concatenated clusters and hierarchical clusters\n",
    "cluster_mapper = df_centroids['hclust_labels'].to_dict()\n",
    "cluster_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85cf22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df.copy()\n",
    "\n",
    "# Mapping the hierarchical clusters on the centroids to the observations\n",
    "df_['merged_labels'] = df_.apply(\n",
    "    lambda row: cluster_mapper[\n",
    "        (row['value_labels'], row['behaviour_labels'])\n",
    "    ], axis=1\n",
    ")\n",
    "\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44780e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged cluster centroids\n",
    "df_.groupby('merged_labels').mean(numeric_only=True)[metric_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0da34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge cluster contigency table\n",
    "# Getting size of each final cluster\n",
    "df_counts = df_.groupby('merged_labels')\\\n",
    "    .size()\\\n",
    "    .to_frame()\n",
    "\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1b014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the product and behavior labels\n",
    "df_counts = df_counts\\\n",
    "    .rename({v:k for k, v in cluster_mapper.items()})\\\n",
    "    .reset_index()\n",
    "\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4271c6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts['value_labels'] = df_counts['merged_labels'].apply(lambda x: x[0])\n",
    "df_counts['behaviour_labels'] = df_counts['merged_labels'].apply(lambda x: x[1])\n",
    "\n",
    "\n",
    "df_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc014a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts.pivot(values=0, index='value_labels', columns='behaviour_labels')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b780ae0e",
   "metadata": {},
   "source": [
    "Keep Manual Merging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "971a9bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_manual.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782e7522",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e47c9af",
   "metadata": {},
   "source": [
    "## \n",
    "<a class=\"anchor\" id=\"analysis\">\n",
    "\n",
    "## 4.2 Cluster Analysis\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "329d6d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_profiles(df, label_columns, figsize, \n",
    "                     cmap=\"tab10\",\n",
    "                     compare_titles=None):\n",
    "    \"\"\"\n",
    "    Pass df with labels columns of one or multiple clustering labels. \n",
    "    Then specify this label columns to perform the cluster profile according to them.\n",
    "    \"\"\"\n",
    "    \n",
    "    if compare_titles == None:\n",
    "        compare_titles = [\"\"]*len(label_columns)\n",
    "        \n",
    "    fig, axes = plt.subplots(nrows=len(label_columns), \n",
    "                             ncols=2, \n",
    "                             figsize=figsize, \n",
    "                             constrained_layout=True,\n",
    "                             squeeze=False)\n",
    "    for ax, label, titl in zip(axes, label_columns, compare_titles):\n",
    "        # Filtering df\n",
    "        drop_cols = [i for i in label_columns if i!=label]\n",
    "        dfax = df.drop(drop_cols, axis=1)\n",
    "        \n",
    "        # Getting the cluster centroids and counts\n",
    "        centroids = dfax.groupby(by=label, as_index=False).mean()\n",
    "        counts = dfax.groupby(by=label, as_index=False).count().iloc[:,[0,1]]\n",
    "        counts.columns = [label, \"counts\"]\n",
    "        \n",
    "        # Setting Data\n",
    "        pd.plotting.parallel_coordinates(centroids, \n",
    "                                            label, \n",
    "                                            color = sns.color_palette(cmap),\n",
    "                                            ax=ax[0])\n",
    "\n",
    "\n",
    "\n",
    "        sns.barplot(x=label, \n",
    "                    hue=label,\n",
    "                    y=\"counts\", \n",
    "                    data=counts, \n",
    "                    ax=ax[1], \n",
    "                    palette=sns.color_palette(cmap),\n",
    "                    legend=False\n",
    "                    )\n",
    "\n",
    "        #Setting Layout\n",
    "        handles, _ = ax[0].get_legend_handles_labels()\n",
    "        cluster_labels = [\"Cluster {}\".format(i) for i in range(len(handles))]\n",
    "        ax[0].annotate(text=titl, xy=(0.95,1.1), xycoords='axes fraction', fontsize=13, fontweight = 'heavy') \n",
    "        ax[0].axhline(color=\"black\", linestyle=\"--\")\n",
    "        ax[0].set_title(\"Cluster Means - {} Clusters\".format(len(handles)), fontsize=13)\n",
    "        ax[0].set_xticklabels(ax[0].get_xticklabels(), \n",
    "                              rotation=40,\n",
    "                              ha='right'\n",
    "                              )\n",
    "        \n",
    "        ax[0].legend(handles, cluster_labels,\n",
    "                     loc='center left', bbox_to_anchor=(1, 0.5), title=label\n",
    "                     ) # Adaptable to number of clusters\n",
    "        \n",
    "        ax[1].set_xticks([i for i in range(len(handles))])\n",
    "        ax[1].set_xticklabels(cluster_labels)\n",
    "        ax[1].set_xlabel(\"\")\n",
    "        ax[1].set_ylabel(\"Absolute Frequency\")\n",
    "        ax[1].set_title(\"Cluster Sizes - {} Clusters\".format(len(handles)), fontsize=13)\n",
    "        \n",
    "        \n",
    "    \n",
    "    # plt.subplots_adjust(hspace=0.4, top=0.90)\n",
    "    plt.suptitle(\"Cluster Simple Profiling\", fontsize=23)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03eb74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profilling each cluster \n",
    "cluster_profiles(\n",
    "    df = df[metric_features + ['value_labels', 'behaviour_labels', 'merged_labels']], \n",
    "    label_columns = ['value_labels', 'behaviour_labels','merged_labels'], \n",
    "    figsize = (28, 13), \n",
    "    compare_titles = [\"Value clustering\", \"Behaviour clustering\", \"Merged clusters\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f19b3b",
   "metadata": {},
   "source": [
    "## \n",
    "<a class=\"anchor\" id=\"profiling\">\n",
    "\n",
    "# 5. Profiling with Categorical features & Unused Features \n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd511494",
   "metadata": {},
   "source": [
    "## City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258cbda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city = df[['merged_labels',\n",
    "            'enc_customer_city_2',\n",
    "            'enc_customer_city_4',\n",
    "            'enc_customer_city_8']].groupby(['merged_labels']).sum()\n",
    "\n",
    "df_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ca8b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['merged_labels']].groupby(['merged_labels']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e037e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, \n",
    "                         df['merged_labels'].nunique() + 1, # Add an extra ax for population countplot\n",
    "                         figsize=(16,4),\n",
    "                         tight_layout=True,\n",
    "                        #  sharey=True,\n",
    "                         )\n",
    "\n",
    "\n",
    "for i in range(len(axes.flatten())): \n",
    "    ax = axes[i]\n",
    "    if i == 0:\n",
    "        sns.countplot(df, \n",
    "                        x='customer_city', \n",
    "                        order = df['customer_city'].value_counts().index,\n",
    "                        ax=ax)\n",
    "        ax.set_title(\"All Data\")\n",
    "        \n",
    "    else:    \n",
    "        sns.countplot(df.loc[df['merged_labels']==i-1], \n",
    "                    x='customer_city', \n",
    "                    order = df['customer_city'].value_counts().index,\n",
    "                    ax=ax)\n",
    "        ax.set_title(\"Cluster {}\".format(i-1))\n",
    "    \n",
    "    ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "plt.suptitle(\"City Counts\", )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6ffc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl_city = df.groupby([\n",
    "    \"merged_labels\", \n",
    "    \"customer_city\",\n",
    "    ])['customer_city'].size().unstack()\n",
    "\n",
    "df_cl_city\n",
    "\n",
    "\n",
    "df_cl_city.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d725edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl_city_pct = df_cl_city.copy()\n",
    "for i in df['customer_city'].unique():\n",
    "    df_cl_city_pct[i] = 100*df_cl_city_pct[i]/df['merged_labels'].value_counts().sort_index().values\n",
    "\n",
    "df_cl_city_pct.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8dec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, df['merged_labels'].nunique(), \n",
    "                         figsize=(12,4),\n",
    "                         sharey=True,)\n",
    "\n",
    "for ax, clust in zip(axes.flatten(), range(df['merged_labels'].nunique())): \n",
    "    df_cl = df.loc[df['merged_labels']==clust]\n",
    "    sns.countplot(df_cl, \n",
    "                  x='customer_city', \n",
    "                  order = df['customer_city'].value_counts().index,\n",
    "                  ax=ax)\n",
    "    \n",
    "    ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "    ax.set_xlabel(\"\")\n",
    "\n",
    "    ax.set_title(\"Cluster {}\".format(clust))\n",
    "plt.suptitle(\"Count of City by Cluster\", y=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6255ffa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df[\"merged_labels\"],df[\"customer_city\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917d8eaa",
   "metadata": {},
   "source": [
    "## Promo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4881c09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_promo = df[['merged_labels',\n",
    "            'enc_last_promo',\n",
    "            ]].groupby(['merged_labels']).sum()\n",
    "\n",
    "df_promo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8373e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['merged_labels']].groupby(['merged_labels']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1326b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, \n",
    "                         df['merged_labels'].nunique() + 1, # Add an extra ax for population countplot\n",
    "                         figsize=(16,4),\n",
    "                         tight_layout=True,\n",
    "                        #  sharey=True,\n",
    "                         )\n",
    "\n",
    "\n",
    "for i in range(len(axes.flatten())): \n",
    "    ax = axes[i]\n",
    "    if i == 0:\n",
    "        sns.countplot(df, \n",
    "                        x='last_promo', \n",
    "                        order = df['last_promo'].value_counts().index,\n",
    "                        ax=ax)\n",
    "        ax.set_title(\"All Data\")\n",
    "        \n",
    "    else:    \n",
    "        sns.countplot(df.loc[df['merged_labels']==i-1], \n",
    "                    x='last_promo', \n",
    "                    order = df['last_promo'].value_counts().index,\n",
    "                    ax=ax)\n",
    "        ax.set_title(\"Cluster {}\".format(i-1))\n",
    "    \n",
    "    ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "plt.suptitle(\"Promo Counts\", )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a0c5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl_promo = df.groupby([\n",
    "    \"merged_labels\", \n",
    "    \"last_promo\",\n",
    "    ])['last_promo'].size().unstack()\n",
    "\n",
    "df_cl_promo\n",
    "\n",
    "\n",
    "df_cl_promo.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8488f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl_promo_pct = df_cl_promo.copy()\n",
    "for i in df['last_promo'].unique():\n",
    "    df_cl_promo_pct[i] = 100*df_cl_promo_pct[i]/df['merged_labels'].value_counts().sort_index().values\n",
    "\n",
    "df_cl_promo_pct.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619a0639",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, df['merged_labels'].nunique(), \n",
    "                         figsize=(12,4),\n",
    "                         sharey=True,)\n",
    "\n",
    "for ax, clust in zip(axes.flatten(), range(df['merged_labels'].nunique())): \n",
    "    df_cl = df.loc[df['merged_labels']==clust]\n",
    "    sns.countplot(df_cl, \n",
    "                  x='last_promo', \n",
    "                  order = df['last_promo'].value_counts().index,\n",
    "                  ax=ax)\n",
    "    \n",
    "    ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "    ax.set_xlabel(\"\")\n",
    "\n",
    "    ax.set_title(\"Cluster {}\".format(clust))\n",
    "plt.suptitle(\"Count of Last Promo by Cluster\", y=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e11d42",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b159a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enc = df[['merged_labels',\n",
    "            'enc_is_chain_0',\n",
    "            'enc_is_chain_1',\n",
    "]].groupby(['merged_labels']).sum()\n",
    "\n",
    "df_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5872ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['merged_labels']].groupby(['merged_labels']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c85343",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, \n",
    "                         df['merged_labels'].nunique() + 1,  # Add an extra ax for population countplot\n",
    "                         figsize=(16, 4),\n",
    "                         tight_layout=True,\n",
    "                         )\n",
    "\n",
    "for i in range(len(axes.flatten())): \n",
    "    ax = axes[i]\n",
    "    if i == 0:\n",
    "        sns.countplot(df, \n",
    "                      x='is_chain', \n",
    "                      order=df['is_chain'].value_counts().index,\n",
    "                      ax=ax)\n",
    "        ax.set_title(\"All Data\")\n",
    "        \n",
    "    else:    \n",
    "        sns.countplot(df.loc[df['merged_labels'] == i-1],  # Filter by 'merged_labels', not 'is_chain'\n",
    "                      x='is_chain', \n",
    "                      order=df['is_chain'].value_counts().index,\n",
    "                      ax=ax)\n",
    "        ax.set_title(f\"Cluster {i-1}\")\n",
    "    \n",
    "    ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "plt.suptitle(\"Chain Counts\", y=1.05)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77faf292",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl_chain = df.groupby([\n",
    "    \"merged_labels\", \n",
    "    \"is_chain\",\n",
    "    ])['is_chain'].size().unstack()\n",
    "\n",
    "df_cl_chain\n",
    "\n",
    "\n",
    "df_cl_chain.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f4e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl_chain_pct = df_cl_chain.copy()\n",
    "for i in df['is_chain'].unique():\n",
    "    df_cl_chain_pct[i] = 100*df_cl_chain_pct[i]/df['merged_labels'].value_counts().sort_index().values\n",
    "\n",
    "df_cl_chain_pct.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fd2f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, df['merged_labels'].nunique(), \n",
    "                         figsize=(12,4),\n",
    "                         sharey=True,)\n",
    "\n",
    "for ax, clust in zip(axes.flatten(), range(df['merged_labels'].nunique())): \n",
    "    df_cl = df.loc[df['merged_labels']==clust]\n",
    "    sns.countplot(df_cl, \n",
    "                  x='is_chain', \n",
    "                  order = df['is_chain'].value_counts().index,\n",
    "                  ax=ax)\n",
    "    \n",
    "    ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "    ax.set_xlabel(\"\")\n",
    "\n",
    "    ax.set_title(\"Cluster {}\".format(clust))\n",
    "plt.suptitle(\"Count of 'is_chain' by Cluster\", y=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538c334d",
   "metadata": {},
   "source": [
    "## Payment Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d393aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pay = df[['merged_labels',\n",
    "            'enc_payment_method_CARD',\n",
    "       'enc_payment_method_CASH', 'enc_payment_method_DIGI',\n",
    "]].groupby(['merged_labels']).sum()\n",
    "\n",
    "df_pay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023cfda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['merged_labels']].groupby(['merged_labels']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58824a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, \n",
    "                         df['merged_labels'].nunique() + 1,  # Add an extra ax for population countplot\n",
    "                         figsize=(16, 4),\n",
    "                         tight_layout=True,\n",
    "                         )\n",
    "\n",
    "for i in range(len(axes.flatten())): \n",
    "    ax = axes[i]\n",
    "    if i == 0:\n",
    "        # Plot for all data\n",
    "        sns.countplot(df, \n",
    "                      x='payment_method', \n",
    "                      order=df['payment_method'].value_counts().index,\n",
    "                      ax=ax)\n",
    "        ax.set_title(\"All Data\")\n",
    "    else:    \n",
    "        # Plot for specific clusters\n",
    "        sns.countplot(df.loc[df['merged_labels'] == i-1],  # Filter by 'merged_labels'\n",
    "                      x='payment_method', \n",
    "                      order=df['payment_method'].value_counts().index,\n",
    "                      ax=ax)\n",
    "        ax.set_title(f\"Cluster {i-1}\")\n",
    "    \n",
    "    ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "plt.suptitle(\"Payment Counts\", y=1.05)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2c2910",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl_pay = df.groupby([\n",
    "    \"merged_labels\", \n",
    "    \"payment_method\",\n",
    "    ])['payment_method'].size().unstack()\n",
    "\n",
    "df_cl_pay\n",
    "\n",
    "\n",
    "df_cl_pay.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3426b787",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl_pay_pct = df_cl_pay.copy()\n",
    "for i in df['payment_method'].unique():\n",
    "    df_cl_pay_pct[i] = 100*df_cl_pay_pct[i]/df['merged_labels'].value_counts().sort_index().values\n",
    "\n",
    "df_cl_pay_pct.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2851e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, df['merged_labels'].nunique(), \n",
    "                         figsize=(12,4),\n",
    "                         sharey=True,)\n",
    "\n",
    "for ax, clust in zip(axes.flatten(), range(df['merged_labels'].nunique())): \n",
    "    df_cl = df.loc[df['merged_labels']==clust]\n",
    "    sns.countplot(df_cl, \n",
    "                  x='payment_method', \n",
    "                  order = df['payment_method'].value_counts().index,\n",
    "                  ax=ax)\n",
    "    \n",
    "    ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "    ax.set_xlabel(\"\")\n",
    "\n",
    "    ax.set_title(\"Cluster {}\".format(clust))\n",
    "plt.suptitle(\"Count of The Payment Method by Cluster\", y=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b0d5ad",
   "metadata": {},
   "source": [
    "## Age Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4428d608",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age = df[['merged_labels',\n",
    "            'enc_age_group',\n",
    "       \n",
    "]].groupby(['merged_labels']).sum()\n",
    "\n",
    "df_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d78343",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['merged_labels']].groupby(['merged_labels']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1579b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, \n",
    "                         df['merged_labels'].nunique() + 1, # Add an extra ax for population countplot\n",
    "                         figsize=(16,4),\n",
    "                         tight_layout=True,\n",
    "                        #  sharey=True,\n",
    "                         )\n",
    "\n",
    "\n",
    "for i in range(len(axes.flatten())): \n",
    "    ax = axes[i]\n",
    "    if i == 0:\n",
    "        sns.countplot(df, \n",
    "                        x='age_group', \n",
    "                        order = df['age_group'].value_counts().index,\n",
    "                        ax=ax)\n",
    "        ax.set_title(\"All Data\")\n",
    "        \n",
    "    else:    \n",
    "        sns.countplot(df.loc[df['merged_labels']==i-1], \n",
    "                    x='age_group', \n",
    "                    order = df['age_group'].value_counts().index,\n",
    "                    ax=ax)\n",
    "        ax.set_title(\"Cluster {}\".format(i-1))\n",
    "    \n",
    "    ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "plt.suptitle(\"Age Group Counts\", )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac43d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl_age = df.groupby([\n",
    "    \"merged_labels\", \n",
    "    \"age_group\",\n",
    "    ])['age_group'].size().unstack()\n",
    "\n",
    "df_cl_age\n",
    "\n",
    "\n",
    "df_cl_age.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7eec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl_age_pct = df_cl_age.copy()\n",
    "for i in df['age_group'].unique():\n",
    "    df_cl_age_pct[i] = 100*df_cl_age_pct[i]/df['merged_labels'].value_counts().sort_index().values\n",
    "\n",
    "df_cl_age_pct.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e4714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, df['merged_labels'].nunique(), \n",
    "                         figsize=(12,4),\n",
    "                         sharey=True,)\n",
    "\n",
    "for ax, clust in zip(axes.flatten(), range(df['merged_labels'].nunique())): \n",
    "    df_cl = df.loc[df['merged_labels']==clust]\n",
    "    sns.countplot(df_cl, \n",
    "                  x='age_group', \n",
    "                  order = df['age_group'].value_counts().index,\n",
    "                  ax=ax)\n",
    "    \n",
    "    ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "    ax.set_xlabel(\"\")\n",
    "\n",
    "    ax.set_title(\"Cluster {}\".format(clust))\n",
    "plt.suptitle(\"Count of The Age Group by Cluster\", y=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0c409f",
   "metadata": {},
   "source": [
    "## Cuisine Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "df94079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previously saved scaler\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "num_feats = scaler.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "38cdf993",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feats = joblib.load('cat_feats.pkl')\n",
    "\n",
    "cat_feats += ['value_labels', 'behaviour_labels', 'merged_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f33169",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all features in df columns and the list num_feats\n",
    "\n",
    "# variables related to spending are:\n",
    "spending_cols = [\n",
    "    'Main Courses',\n",
    "    'Snacks and Street Food',\n",
    "    'Desserts and Beverages',\n",
    "    'Healthy and Special Diets',\n",
    "    'Other'\n",
    "]\n",
    "\n",
    "cols_to_scale = list(set(df.columns).intersection(num_feats))\n",
    "\n",
    "original_val = df.copy()\n",
    "\n",
    "for col in cols_to_scale:\n",
    "   original_val[col] = scaler[col].inverse_transform(pd.DataFrame (df[col]))\n",
    "\n",
    "\n",
    "threshold = 1e-5\n",
    "for col in cols_to_scale:\n",
    "   original_val[col] = original_val[col].apply(lambda x: 0 if abs(x) < threshold else x)\n",
    "\n",
    "\n",
    "# Replace the scaled columns in the original DataFrame\n",
    "df[spending_cols] = original_val[spending_cols]\n",
    "\n",
    "# Calculate the cluster profile with the original spending values\n",
    "cluster_profile_original = df.groupby('merged_labels')[spending_cols].agg(['mean', 'std', 'min', 'max', 'median'])\n",
    "\n",
    "\n",
    "\n",
    "# Display the cluster profile with the original spending values\n",
    "cluster_profile_original.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b721eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean values for the non-scaled spending columns\n",
    "cluster_means_original = cluster_profile_original.xs('mean', axis=1, level=1)\n",
    "\n",
    "# Plot the grouped bar chart for the original (non-scaled) values\n",
    "cluster_means_original.T.plot(kind='bar', figsize=(10, 6), colormap='tab10', edgecolor='black')\n",
    "plt.title('Average Spending by Cuisine Type in Each Cluster (Original Values)')\n",
    "plt.xlabel('Cuisine Types')\n",
    "plt.ylabel('Average Spending')\n",
    "plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941ccd5b",
   "metadata": {},
   "source": [
    "## Time Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434c9517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables related to time are:\n",
    "time_cols = [\n",
    "'early_morning(0h-5h)',\n",
    " 'morning(6h-11h)',\n",
    " 'afternoon(12h-17h)',\n",
    " 'night(18h-23h)'\n",
    "]\n",
    "\n",
    "\n",
    "# Replace the scaled columns in the original DataFrame\n",
    "df[time_cols] = original_val[time_cols]\n",
    "\n",
    "# Calculate the cluster profile with the original time values\n",
    "cluster_profile_original_t = df.groupby('merged_labels')[time_cols].agg(['mean', 'std', 'min', 'max', 'median'])\n",
    "\n",
    "# Display the cluster profile with the original time values\n",
    "cluster_profile_original_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6260859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate means\n",
    "cluster_means_time = df.groupby('merged_labels')[time_cols].mean()\n",
    "\n",
    "# Plot the grouped bar chart\n",
    "cluster_means_time.T.plot(kind='bar', figsize=(10, 6), colormap='tab10', edgecolor='black')\n",
    "plt.title('Average Orders in the Time period in Each Cluster')\n",
    "plt.xlabel('Time Periods')\n",
    "plt.ylabel('Average Orders')\n",
    "plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d05c7f",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015a280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_dim = TSNE(random_state=42).fit_transform(df[metric_features])\n",
    "\n",
    "pd.DataFrame(two_dim).plot.scatter(x=0, y=1, c=df['merged_labels'], colormap='tab10', figsize=(15,10))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b30c83e",
   "metadata": {},
   "source": [
    "## \n",
    "<a class=\"anchor\" id=\"label\">\n",
    "\n",
    "# 6. Label Outliers\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7ba96434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ss_variables(df):\n",
    "    \"\"\"Get the SS for each variable\n",
    "    \"\"\"\n",
    "    ss_vars = df.var() * (df.count() - 1)\n",
    "    return ss_vars\n",
    "\n",
    "def r2_variables(df, labels):\n",
    "    \"\"\"Get the R² for each variable\n",
    "    \"\"\"\n",
    "    sst_vars = get_ss_variables(df)\n",
    "    ssw_vars = np.sum(df.groupby(labels).apply(get_ss_variables))\n",
    "    return 1 - ssw_vars/sst_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27f073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are essentially decomposing the R² into the R² for each variable\n",
    "r2_variables(df[metric_features + ['merged_labels']], 'merged_labels').drop('merged_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea86165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "X = df[metric_features]\n",
    "y = df.merged_labels\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Fitting the decision tree\n",
    "dt = DecisionTreeClassifier(random_state=42, max_depth=3)\n",
    "dt.fit(X_train, y_train)\n",
    "print(\"It is estimated that in average, we are able to predict {0:.2f}% of the customers correctly\".format(dt.score(X_test, y_test)*100))\n",
    "\n",
    "\n",
    "# Performance on training data\n",
    "train_score = dt.score(X_train, y_train)\n",
    "# Performance on test data\n",
    "test_score = dt.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_score * 100:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_score * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307debb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing feature importance\n",
    "pd.Series(dt.feature_importances_, index=X_train.columns).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367be0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the cluster labels of the outliers\n",
    "df_out['merged_labels'] = dt.predict(df_out[metric_features])\n",
    "df_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "619367eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate df_out with the predicted 'merged_labels' back to the original df\n",
    "df = pd.concat([df, df_out[['merged_labels']]], axis=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
