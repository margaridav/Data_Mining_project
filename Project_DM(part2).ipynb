{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a17590b",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"import\">\n",
    "\n",
    "## 1. Import \n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8f9eb9",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"libraries\">\n",
    "\n",
    "## 1.1 Import libraries\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0d5583",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install minisom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "7c5f581a-9c83-4617-8f24-5fbcf514fd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import ceil\n",
    "\n",
    "from itertools import product\n",
    "from scipy.stats import skewnorm\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from minisom import MiniSom\n",
    "## Import Matplotlib functions to create MiniSOM visualizations\n",
    "\n",
    "from matplotlib.patches import RegularPolygon, Ellipse\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib import cm, colorbar\n",
    "from matplotlib import colors as mpl_colors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import __version__ as mplver\n",
    "\n",
    "from sklearn.cluster import MeanShift, DBSCAN, estimate_bandwidth\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "\n",
    "# for better resolution plots\n",
    "%config InlineBackend.figure_format = 'retina' # optionally, you can change 'svg' to 'retina'\n",
    "\n",
    "# Setting seaborn style\n",
    "sns.set()\n",
    "\n",
    "# Display all the df and results\n",
    "pd.set_option('display.max_rows', None)  \n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.width', 1000)  \n",
    "pd.set_option('display.colheader_justify', 'center')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f767083",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"dataset\">\n",
    "\n",
    "## 1.2 Import the dataset\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "32603658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv(\"data_beforeclustering.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "d12bdd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backup=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "8dd55ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.set_index(\"customer_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fd337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feee8e68",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"clustering\">\n",
    "\n",
    "# 2. Clustering\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4b54d9",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"select\">\n",
    "\n",
    "## 2.1 Feature Selection\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f24ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6466a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting feature names into groups\n",
    "metric_features = ['customer_age', 'vendor_count', 'product_count','first_order', 'last_order',\n",
    "                   'CUI_American', 'CUI_Asian', 'CUI_Beverages', 'CUI_Cafe','CUI_Chicken Dishes', \n",
    "                   'CUI_Chinese', 'CUI_Desserts', 'CUI_Healthy','CUI_Indian', 'CUI_Italian', \n",
    "                   'CUI_Japanese', 'CUI_Noodle Dishes','CUI_OTHER', 'CUI_Street Food / Snacks', 'CUI_Thai',\n",
    "                   'Sunday','Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday',\n",
    "                   'HR_0', 'HR_1', 'HR_2', 'HR_3', 'HR_4', 'HR_5', 'HR_6', 'HR_7','HR_8', 'HR_9', \n",
    "                   'HR_10', 'HR_11', 'HR_12', 'HR_13', 'HR_14','HR_15', 'HR_16', 'HR_17', 'HR_18', \n",
    "                   'HR_19', 'HR_20', 'HR_21','HR_22', 'HR_23', 'early_morning(0h-5h)', 'morning(6h-11h)',\n",
    "                   'afternoon(12h-17h)', 'night(18h-23h)',\n",
    "                   'Sum_of_Orders', 'recency', 'active_period', 'frequency','total_spend', 'cuisine_diversity',\n",
    "                   'Weekdays','Weekends', \n",
    "                   'Main Courses','Snacks and Street Food', 'Desserts and Beverages','Healthy and Special Diets', 'Other']\n",
    "\n",
    "non_metric_features = df.columns[df.columns.str.startswith('enc_')].tolist() \n",
    "unused_features = [i for i in df.columns if i not in (metric_features+non_metric_features) ]\n",
    "\n",
    "# Display\n",
    "print('metric_features:', metric_features)\n",
    "print('\\nnon_metric_features:', non_metric_features)\n",
    "print('\\nunused_features:', unused_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b54a96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(24, 18))  # Increased size for clarity\n",
    "\n",
    "# Compute Spearman correlation\n",
    "corr = np.round(df[metric_features].corr(method=\"spearman\"), decimals=2)\n",
    "\n",
    "# Mask for upper triangle and annotations threshold\n",
    "mask_annot = np.absolute(corr.values) >= 0.5\n",
    "annot = np.where(mask_annot, corr.values, np.full(corr.shape, \"\"))  # Annotate only significant correlations\n",
    "\n",
    "# Mask upper triangle\n",
    "matrix = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Customize the heatmap\n",
    "sns.heatmap(\n",
    "    data=corr,\n",
    "    annot=annot,                   # Show annotations\n",
    "    mask=matrix,                   # Apply mask to hide upper triangle\n",
    "    cmap='PiYG',               # Adjust colormap for better clarity\n",
    "    fmt='',                        # Avoid formatting issues\n",
    "    annot_kws={\"size\": 8},        # Font size for annotations\n",
    "    vmin=-1, vmax=1, center=0,     # Set limits for the colormap\n",
    "    square=True, linewidths=.5,    # Keep square cells with a border\n",
    "    cbar_kws={\"shrink\": 0.8}       # Shrink color bar for better fit\n",
    ")\n",
    "\n",
    "# Title and labels\n",
    "fig.subplots_adjust(top=0.9)       # Adjust layout to fit the title\n",
    "fig.suptitle(\"Correlation Matrix\", fontsize=22, weight='bold')  # Bold, larger title\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)  # Rotate x-axis labels\n",
    "plt.yticks(rotation=0, fontsize=12)               # Keep y-axis labels horizontal\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e858c6",
   "metadata": {},
   "source": [
    "Drop from dataset HR, CUI and DOW variables since we have them explicit in other features\n",
    "\n",
    "- HR - Time periods\n",
    "- CUI - Types of cuisine\n",
    "- DOW - Week days and Weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "2c96af80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = ['HR_0', 'HR_1', 'HR_2', 'HR_3', 'HR_4', 'HR_5', 'HR_6', 'HR_7', 'HR_8', 'HR_9', \n",
    "                   'HR_10', 'HR_11', 'HR_12', 'HR_13', 'HR_14', 'HR_15', 'HR_16', 'HR_17', 'HR_18', \n",
    "                   'HR_19', 'HR_20', 'HR_21', 'HR_22', 'HR_23',\n",
    "                   'Sunday','Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday',\n",
    "                   'CUI_American', 'CUI_Asian', 'CUI_Beverages', 'CUI_Cafe','CUI_Chicken Dishes', \n",
    "                   'CUI_Chinese', 'CUI_Desserts', 'CUI_Healthy','CUI_Indian', 'CUI_Italian', \n",
    "                   'CUI_Japanese', 'CUI_Noodle Dishes','CUI_OTHER', 'CUI_Street Food / Snacks', 'CUI_Thai']\n",
    "\n",
    "metric_features = [i for i in metric_features if i not in columns_to_drop]\n",
    "\n",
    "df=df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f8b5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 14))  # Increased size for clarity\n",
    "\n",
    "# Compute Spearman correlation\n",
    "corr = np.round(df[metric_features].corr(method=\"spearman\"), decimals=2)\n",
    "\n",
    "# Mask for upper triangle and annotations threshold\n",
    "mask_annot = np.absolute(corr.values) >= 0.5\n",
    "annot = np.where(mask_annot, corr.values, np.full(corr.shape, \"\"))  # Annotate only significant correlations\n",
    "\n",
    "# Mask upper triangle\n",
    "matrix = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Customize the heatmap\n",
    "sns.heatmap(\n",
    "    data=corr,\n",
    "    annot=annot,                   # Show annotations\n",
    "    mask=matrix,                   # Apply mask to hide upper triangle\n",
    "    cmap='PiYG',               # Adjust colormap for better clarity\n",
    "    fmt='',                        # Avoid formatting issues\n",
    "    annot_kws={\"size\": 8},        # Font size for annotations\n",
    "    vmin=-1, vmax=1, center=0,     # Set limits for the colormap\n",
    "    square=True, linewidths=.5,    # Keep square cells with a border\n",
    "    cbar_kws={\"shrink\": 0.8}       # Shrink color bar for better fit\n",
    ")\n",
    "\n",
    "# Title and labels\n",
    "fig.subplots_adjust(top=0.9)       # Adjust layout to fit the title\n",
    "fig.suptitle(\"Correlation Matrix\", fontsize=22, weight='bold')  # Bold, larger title\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)  # Rotate x-axis labels\n",
    "plt.yticks(rotation=0, fontsize=12)               # Keep y-axis labels horizontal\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "8fd7838f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select variables according to their correlations\n",
    "# # Updating metric_features\n",
    "metric_features.remove('vendor_count') #cuisine_diversity and weekdays and active_period and Sum_of_orders\n",
    "metric_features.remove('product_count') #total_spend and weekdays and active_period and Sum_of_orders\n",
    "metric_features.remove('active_period') #Sum_of_orders; aux variable to calculate Frequency\n",
    "metric_features.remove('last_order') #recency\n",
    "metric_features.remove('first_order')\n",
    "#first_order and last_order used to calculate active_period\n",
    "metric_features.remove('Sum_of_Orders')\n",
    "metric_features.remove('cuisine_diversity')\n",
    "\n",
    "\n",
    "unused_features.extend(['active_period','last_order','cuisine_diversity', 'Sum_of_Orders', 'first_order','product_count','vendor_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb282b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(24, 18))  # Increased size for clarity\n",
    "\n",
    "# Compute Spearman correlation\n",
    "corr = np.round(df[metric_features].corr(method=\"spearman\"), decimals=2)\n",
    "\n",
    "# Mask for annotations near 0 (e.g., absolute correlation < 0.2)\n",
    "mask_annot = (np.absolute(corr.values) < 0.2)  # You can adjust the threshold to be closer to 0\n",
    "annot = np.where(mask_annot, corr.values, np.full(corr.shape, \"\"))  # Annotate only near-zero correlations\n",
    "\n",
    "# Mask upper triangle\n",
    "matrix = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Customize the heatmap\n",
    "sns.heatmap(\n",
    "    data=corr,\n",
    "    annot=annot,                   # Show annotations for near-zero correlations\n",
    "    mask=matrix,                   # Apply mask to hide upper triangle\n",
    "    cmap='PiYG',                   # Adjust colormap for better clarity\n",
    "    fmt='',                        # Avoid formatting issues\n",
    "    annot_kws={\"size\": 8},         # Font size for annotations\n",
    "    vmin=-1, vmax=1, center=0,     # Set limits for the colormap\n",
    "    square=True, linewidths=.5,    # Keep square cells with a border\n",
    "    cbar_kws={\"shrink\": 0.8}       # Shrink color bar for better fit\n",
    ")\n",
    "\n",
    "# Title and labels\n",
    "fig.subplots_adjust(top=0.9)       # Adjust layout to fit the title\n",
    "fig.suptitle(\"Correlation Matrix (Near Zero)\", fontsize=22, weight='bold')  # Bold, larger title\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)  # Rotate x-axis labels\n",
    "plt.yticks(rotation=0, fontsize=12)               # Keep y-axis labels horizontal\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3478a057",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* Customer_age does not seem to have a relevant correlation to any variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "4a572c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_features.remove('customer_age')\n",
    "unused_features.extend(['customer_age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112abfd4",
   "metadata": {},
   "source": [
    "## Visualizing data with SOMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1543a5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the total number of neurons to decide the Grid Size\n",
    "total_neurons = 5 * (df.shape[0])**0.5\n",
    "\n",
    "print(f'The total number of neurons is: {total_neurons}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad460d34",
   "metadata": {},
   "source": [
    "* Considering the total number of neurons equal to 900, and M equal to N:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64b83a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 900**0.5\n",
    "print(f'M and N = {size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf1e30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 30 \n",
    "N = 30\n",
    "neighborhood_function = \"gaussian\"\n",
    "topology = \"hexagonal\"\n",
    "n_feats = len(metric_features)\n",
    "learning_rate = 0.6\n",
    "\n",
    "\n",
    "som_data = df[metric_features].values\n",
    "\n",
    "sm = MiniSom(M, N,              # 30x30 map size\n",
    "             n_feats,           # Number of the elements of the vectors in input.\n",
    "             learning_rate=learning_rate, \n",
    "             topology=topology, \n",
    "             neighborhood_function=neighborhood_function, \n",
    "             activation_distance='euclidean',\n",
    "             random_seed=42\n",
    "             )\n",
    "\n",
    "# Initializes the weights of the SOM picking random samples from data.\n",
    "sm.random_weights_init(som_data) \n",
    "\n",
    "\n",
    "print(\"Before training:\")\n",
    "print(\"QE\", np.round(sm.quantization_error(som_data),4))\n",
    "print(\"TE\", np.round(sm.topographic_error(som_data),4))\n",
    "\n",
    "\n",
    "\n",
    "# Trains the SOM using all the vectors in data sequentially\n",
    "# minisom does not distinguish between unfolding and fine tuning phase;\n",
    "\n",
    "sm.train_batch(som_data, 300000)\n",
    "\n",
    "print(\"After training:\")\n",
    "print(\"QE\", np.round(sm.quantization_error(som_data),4))\n",
    "print(\"TE\", np.round(sm.topographic_error(som_data),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cd28b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = sm.get_weights()\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "5f9991eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hexagons(som,              # Trained SOM model \n",
    "                  sf,               # matplotlib figure object\n",
    "                  colornorm,        # colornorm\n",
    "                  matrix_vals,      # SOM weights or\n",
    "                  label=\"\",         # title for figure\n",
    "                  cmap=cm.Grays,    # colormap to use\n",
    "                  annot=False       \n",
    "                  ):\n",
    "\n",
    "    \n",
    "    axs = sf.subplots(1,1)\n",
    "    \n",
    "    for i in range(matrix_vals.shape[0]):\n",
    "        for j in range(matrix_vals.shape[1]):\n",
    "\n",
    "            wx, wy = som.convert_map_to_euclidean((i,j)) \n",
    "\n",
    "            hex = RegularPolygon((wx, wy), \n",
    "                                numVertices=6, \n",
    "                                radius= np.sqrt(1/3),\n",
    "                                facecolor=cmap(colornorm(matrix_vals[i, j])), \n",
    "                                alpha=1, \n",
    "                                edgecolor='white',\n",
    "                                linewidth=.5)\n",
    "            axs.add_patch(hex)\n",
    "            if annot==True:\n",
    "                annot_val = np.round(matrix_vals[i,j],2)\n",
    "                if int(annot_val) == annot_val:\n",
    "                    annot_val = int(annot_val)\n",
    "                axs.text(wx,wy, annot_val, \n",
    "                        ha='center', va='center', \n",
    "                        fontsize='x-small')\n",
    "\n",
    "\n",
    "    ## Remove axes for hex plot\n",
    "    axs.margins(.05)\n",
    "    axs.set_aspect('equal')\n",
    "    axs.axis(\"off\")\n",
    "    axs.set_title(label)\n",
    "\n",
    "    \n",
    "\n",
    "    # ## Add colorbar\n",
    "    divider = make_axes_locatable(axs)\n",
    "    ax_cb = divider.append_axes(\"right\", size=\"5%\", pad=\"0%\")\n",
    "\n",
    "    ## Create a Mappable object\n",
    "    cmap_sm = plt.cm.ScalarMappable(cmap=cmap, norm=colornorm)\n",
    "    cmap_sm.set_array([])\n",
    "\n",
    "    ## Create custom colorbar \n",
    "    cb1 = colorbar.Colorbar(ax_cb,\n",
    "                            orientation='vertical', \n",
    "                            alpha=1,\n",
    "                            mappable=cmap_sm\n",
    "                            )\n",
    "    cb1.ax.get_yaxis().labelpad = 6\n",
    "\n",
    "    # Add colorbar to plot\n",
    "    sf.add_axes(ax_cb)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return sf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412506af",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(metric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e589b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Plot Component Planes\n",
    "##############################\n",
    "\n",
    "figsize=(20,20)\n",
    "fig = plt.figure(figsize=figsize, constrained_layout=True, dpi=128, )\n",
    "\n",
    "subfigs = fig.subfigures(5,3,wspace=.15)\n",
    "\n",
    "colornorm = mpl_colors.Normalize(vmin=np.min(weights), vmax=np.max(weights))\n",
    "\n",
    "for cpi, sf in zip(range(len(metric_features)), subfigs.flatten()):\n",
    "    \n",
    "    matrix_vals = weights[:,:,cpi]\n",
    "    vext = np.max(np.abs([np.min(matrix_vals), np.max(matrix_vals)]))\n",
    "    colornorm = mpl_colors.Normalize(vmin=np.min(matrix_vals), vmax=np.max(matrix_vals))\n",
    "    # colornorm = mpl_colors.CenteredNorm(vcenter=0, halfrange=vext)\n",
    "\n",
    "\n",
    "    sf = plot_hexagons(sm, sf, \n",
    "                    colornorm,\n",
    "                    matrix_vals,\n",
    "                    label=metric_features[cpi],\n",
    "                    cmap=cm.coolwarm,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ba7327",
   "metadata": {},
   "outputs": [],
   "source": [
    "umatrix = sm.distance_map(scaling='mean')\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "\n",
    "colornorm = mpl_colors.Normalize(vmin=np.min(umatrix), vmax=np.max(umatrix))\n",
    "\n",
    "fig = plot_hexagons(sm, fig, \n",
    "                    colornorm,\n",
    "                    umatrix,\n",
    "                    label=\"U-matrix\",\n",
    "                    cmap=cm.RdYlBu_r,\n",
    "                    annot=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58211f1c",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"DBSCAN\">\n",
    "\n",
    "## 2.2 DBSCAN for Outliers\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining eps and min_samples:\n",
    "- **MinPts**: As a rule of thumb, **minPts = 2 x dim** can be used.\n",
    "\n",
    "- **ε**: The value for ε can then be chosen by using a **k-distance graph**, plotting the distance to the kth (k = minPts - 1) nearest neighbor ordered from the largest to the smallest value. Good values of ε are where this plot shows an **\"elbow\"**: if ε is chosen much too small, a large part of the data will not be clustered; whereas for a too high value of ε, clusters will merge and the majority of objects will be in the same cluster. \n",
    "\n",
    "- The assumption is that for points in a cluster, **their k nearest neighbors are at roughly the same distance**. Noise points have their k-th nearest neighbors at farther distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13dc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding dim\n",
    "dim=len(metric_features)\n",
    "print(f'dim = {dim}')\n",
    "print(f'MinPts = {2*dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd73649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute k-distances\n",
    "n_neighbors = (dim*2)-1  # Number of neighbors to consider (MinPts-1)\n",
    "neigh = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "neigh.fit(df[metric_features])\n",
    "distances, _ = neigh.kneighbors(df[metric_features])\n",
    "\n",
    "# Sort distances for plotting\n",
    "distances = np.sort(distances[:, -1])\n",
    "\n",
    "# Plot k-distance graph\n",
    "plt.figure(figsize=(8, 6)) \n",
    "plt.plot(distances, color='steelblue', linewidth=2, label=f\"{n_neighbors}th Nearest Neighbor Distance\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"K-Distance Graph to Determine Optimal Epsilon\", fontsize=16, pad=15)\n",
    "plt.xlabel(\"Points (Sorted by Distance)\", fontsize=14, labelpad=10)\n",
    "plt.ylabel(f\"Distance to {n_neighbors}th Nearest Neighbor\", fontsize=14, labelpad=10)\n",
    "\n",
    "# Add a grid\n",
    "plt.grid(visible=True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# Highlight the \"elbow\" area\n",
    "plt.axhline(y=7, color='red', linestyle=\"--\", linewidth=1.5, label=\"Potential Elbow Region\")\n",
    "plt.axhline(y=2, color='red', linestyle=\"--\", linewidth=1.5)\n",
    "plt.axhline(y=4, color='green', linestyle=\"--\", linewidth=1.5)\n",
    "\n",
    "# Customize ticks\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(fontsize=12, loc=\"upper left\")\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292b604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=4, min_samples=dim*2, n_jobs=-1)\n",
    "dbscan_labels = dbscan.fit_predict(df[metric_features])\n",
    "\n",
    "dbscan_n_clusters = len(np.unique(dbscan_labels))\n",
    "print(\"Number of estimated clusters : %d\" % dbscan_n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d859e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(dbscan_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65d6112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the labels to df\n",
    "df_concat = pd.concat([df[metric_features], pd.Series(dbscan_labels, index=df.index, name=\"dbscan_labels\")], axis=1)\n",
    "df_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33031629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting noise (potential outliers)\n",
    "num_noise = len(df_concat.loc[df_concat['dbscan_labels'] == -1])\n",
    "total_points = len(df_concat)\n",
    "percent_outliers = (num_noise / total_points) * 100\n",
    "\n",
    "print(num_noise)\n",
    "print(f\"Percentage of potencial outliers: {percent_outliers:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "2589f57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ss(df):\n",
    "    \"\"\"Computes the sum of squares for all variables given a dataset\n",
    "    \"\"\"\n",
    "    ss = np.sum(df.var() * (df.count() - 1))\n",
    "    return ss  # return sum of sum of squares of each df variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13960401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the R^2 of the cluster solution\n",
    "df_nonoise = df_concat.loc[df_concat['dbscan_labels'] != -1]\n",
    "sst = get_ss(df[metric_features])  # get total sum of squares\n",
    "ssw_labels = df_nonoise.groupby(by='dbscan_labels').apply(get_ss)  # compute ssw for each cluster labels\n",
    "ssb = sst - np.sum(ssw_labels)  # SST = SSW + SSB\n",
    "r2 = ssb / sst\n",
    "print(\"Cluster solution with R^2 of %0.4f\" % r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be11789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to reduce to 2D\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(df_concat[metric_features])  # Use your features for clustering\n",
    "\n",
    "# Create a new dataframe with PCA components and cluster labels\n",
    "df_concat['PCA1'] = pca_result[:, 0]\n",
    "df_concat['PCA2'] = pca_result[:, 1]\n",
    "\n",
    "# Scatter plot of PCA components with cluster labels\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=df_concat, x='PCA1', y='PCA2', hue='dbscan_labels', palette='PiYG', legend='full')\n",
    "\n",
    "# Highlight outliers\n",
    "outliers = df_concat[df_concat['dbscan_labels'] == -1]\n",
    "plt.scatter(outliers['PCA1'], outliers['PCA2'], color='red', label='Outliers', marker='x')\n",
    "\n",
    "plt.title('DBSCAN Clustering with Outliers (PCA)')\n",
    "plt.xlabel('PCA1')\n",
    "plt.ylabel('PCA2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "c96ab586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the newly detected outliers (they will be classified later based on the final clusters)\n",
    "df_out = df[dbscan_labels==-1].copy()\n",
    "\n",
    "# New df without outliers \n",
    "df = df[dbscan_labels!=-1].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8d762f",
   "metadata": {},
   "source": [
    "## Clustering by perspectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd1906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "ac0fd789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Value-Based Segmentation\n",
    "value_based_features = ['total_spend', 'frequency', 'recency']\n",
    "\n",
    "# 2. Preference-Based Segmentation\n",
    "preference_based_features = ['early_morning(0h-5h)', 'morning(6h-11h)', 'afternoon(12h-17h)', 'night(18h-23h)', 'Weekdays', 'Weekends',\n",
    "                             'Main Courses','Snacks and Street Food', 'Desserts and Beverages','Healthy and Special Diets', 'Other']\n",
    "\n",
    "\n",
    "df_value = df[value_based_features].copy()\n",
    "df_prf = df[preference_based_features].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "ec4149b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the clusterers\n",
    "kmeans = KMeans(\n",
    "    init='k-means++',\n",
    "    n_init=20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "hierarchical = AgglomerativeClustering(\n",
    "    metric='euclidean'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "e4042eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ss(df, feats):\n",
    "    \"\"\"\n",
    "    Calculate the sum of squares (SS) for the given DataFrame.\n",
    "\n",
    "    The sum of squares is computed as the sum of the variances of each column\n",
    "    multiplied by the number of non-NA/null observations minus one.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame for which the sum of squares is to be calculated.\n",
    "    feats (list of str): A list of feature column names to be used in the calculation.\n",
    "\n",
    "    Returns:\n",
    "    float: The sum of squares of the DataFrame.\n",
    "    \"\"\"\n",
    "    df_ = df[feats]\n",
    "    ss = np.sum(df_.var() * (df_.count() - 1))\n",
    "    \n",
    "    return ss \n",
    "\n",
    "\n",
    "def get_ssb(df, feats, label_col):\n",
    "    \"\"\"\n",
    "    Calculate the between-group sum of squares (SSB) for the given DataFrame.\n",
    "    The between-group sum of squares is computed as the sum of the squared differences\n",
    "    between the mean of each group and the overall mean, weighted by the number of observations\n",
    "    in each group.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame containing the data.\n",
    "    feats (list of str): A list of feature column names to be used in the calculation.\n",
    "    label_col (str): The name of the column in the DataFrame that contains the group labels.\n",
    "    \n",
    "    Returns\n",
    "    float: The between-group sum of squares of the DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    ssb_i = 0\n",
    "    for i in np.unique(df[label_col]):\n",
    "        df_ = df.loc[:, feats]\n",
    "        X_ = df_.values\n",
    "        X_k = df_.loc[df[label_col] == i].values\n",
    "        \n",
    "        ssb_i += (X_k.shape[0] * (np.square(X_k.mean(axis=0) - X_.mean(axis=0))) )\n",
    "\n",
    "    ssb = np.sum(ssb_i)\n",
    "    \n",
    "\n",
    "    return ssb\n",
    "\n",
    "\n",
    "def get_ssw(df, feats, label_col):\n",
    "    \"\"\"\n",
    "    Calculate the sum of squared within-cluster distances (SSW) for a given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame containing the data.\n",
    "    feats (list of str): A list of feature column names to be used in the calculation.\n",
    "    label_col (str): The name of the column containing cluster labels.\n",
    "\n",
    "    Returns:\n",
    "    float: The sum of squared within-cluster distances (SSW).\n",
    "    \"\"\"\n",
    "    feats_label = feats+[label_col]\n",
    "\n",
    "    df_k = df[feats_label].groupby(by=label_col).apply(lambda col: get_ss(col, feats), \n",
    "                                                       include_groups=False)\n",
    "\n",
    "    return df_k.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "831d4985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rsq(df, feats, label_col):\n",
    "    \"\"\"\n",
    "    Calculate the R-squared value for a given DataFrame and features.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing the data.\n",
    "    feats (list): A list of feature column names to be used in the calculation.\n",
    "    label_col (str): The name of the column containing the labels or cluster assignments.\n",
    "\n",
    "    Returns:\n",
    "    float: The R-squared value, representing the proportion of variance explained by the clustering.\n",
    "    \"\"\"\n",
    "\n",
    "    df_sst_ = get_ss(df, feats)                 # get total sum of squares\n",
    "    df_ssw_ = get_ssw(df, feats, label_col)     # get ss within\n",
    "    df_ssb_ = df_sst_ - df_ssw_                 # get ss between\n",
    "\n",
    "    # r2 = ssb/sst \n",
    "    return (df_ssb_/df_sst_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "442594ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_r2_scores(df, feats, clusterer, min_k=2, max_k=10):\n",
    "    \"\"\"\n",
    "    Loop over different values of k. To be used with sklearn clusterers.\n",
    "    \"\"\"\n",
    "    r2_clust = {}\n",
    "    for n in range(min_k, max_k):\n",
    "        clust = clone(clusterer).set_params(n_clusters=n)\n",
    "        labels = clust.fit_predict(df)\n",
    "        df_concat = pd.concat([df, \n",
    "                               pd.Series(labels, name='labels', index=df.index)], axis=1)  \n",
    "\n",
    "        r2_clust[n] = get_rsq(df_concat, feats, 'labels' )\n",
    "    return r2_clust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bf7c76",
   "metadata": {},
   "source": [
    "### 1. Value-Based "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26601c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# range_clusters = range(1, 11)\n",
    "# inertia = []\n",
    "# for n_clus in range_clusters:  # iterate over desired ncluster range\n",
    "#     kmclust = KMeans(n_clusters=n_clus, init='k-means++', n_init=15, random_state=1)\n",
    "#     kmclust.fit(df_value)\n",
    "#     inertia.append(kmclust.inertia_)  # save the inertia of the given cluster solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "76932083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The inertia plot\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(9,5))\n",
    "\n",
    "# ax.plot(range_clusters, inertia)\n",
    "# ax.set_xticks(range_clusters)\n",
    "# ax.set_ylabel(\"Inertia: SSw\")\n",
    "# ax.set_xlabel(\"Number of clusters\")\n",
    "# ax.set_title(\"Inertia plot over clusters\", size=15)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08bef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Storing average silhouette metric\n",
    "# avg_silhouette = []\n",
    "# for nclus in range_clusters:\n",
    "#     # Skip nclus == 1\n",
    "#     if nclus == 1:\n",
    "#         continue\n",
    "    \n",
    "#     # Create a figure\n",
    "#     fig = plt.figure(figsize=(13, 7))\n",
    "\n",
    "#     # Initialize the KMeans object with n_clusters value and a random generator\n",
    "#     # seed of 10 for reproducibility.\n",
    "#     kmclust = KMeans(n_clusters=nclus, init='k-means++', n_init=15, random_state=1)\n",
    "#     cluster_labels = kmclust.fit_predict(df_value)\n",
    "\n",
    "#     # The silhouette_score gives the average value for all the samples.\n",
    "#     # This gives a perspective into the density and separation of the formed clusters\n",
    "#     silhouette_avg = silhouette_score(df_value, cluster_labels)\n",
    "#     avg_silhouette.append(silhouette_avg)\n",
    "#     print(f\"For n_clusters = {nclus}, the average silhouette_score is : {silhouette_avg}\")\n",
    "\n",
    "#     # Compute the silhouette scores for each sample\n",
    "#     sample_silhouette_values = silhouette_samples(df_value, cluster_labels)\n",
    "\n",
    "#     y_lower = 10\n",
    "#     for i in range(nclus):\n",
    "#         # Aggregate the silhouette scores for samples belonging to cluster i, and sort them\n",
    "#         ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "#         ith_cluster_silhouette_values.sort()\n",
    "        \n",
    "#         # Get y_upper to demarcate silhouette y range size\n",
    "#         size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "#         y_upper = y_lower + size_cluster_i\n",
    "        \n",
    "#         # Filling the silhouette\n",
    "#         color = cm.nipy_spectral(float(i) / nclus)\n",
    "#         plt.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "#                           0, ith_cluster_silhouette_values,\n",
    "#                           facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "#         # Label the silhouette plots with their cluster numbers at the middle\n",
    "#         plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "#         # Compute the new y_lower for next plot\n",
    "#         y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "#     plt.title(\"The silhouette plot for the various clusters.\")\n",
    "#     plt.xlabel(\"The silhouette coefficient values\")\n",
    "#     plt.ylabel(\"Cluster label\")\n",
    "\n",
    "#     # The vertical line for average silhouette score of all the values\n",
    "#     plt.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    \n",
    "#     # The silhouette coefficient can range from -1, 1\n",
    "#     xmin, xmax = np.round(sample_silhouette_values.min() -0.1, 2), np.round(sample_silhouette_values.max() + 0.1, 2)\n",
    "#     plt.xlim([xmin, xmax])\n",
    "    \n",
    "#     # The (nclus+1)*10 is for inserting blank space between silhouette\n",
    "#     # plots of individual clusters, to demarcate them clearly.\n",
    "#     plt.ylim([0, len(df_value) + (nclus + 1) * 10])\n",
    "\n",
    "#     plt.yticks([])  # Clear the yaxis labels / ticks\n",
    "#     plt.xticks(np.arange(xmin, xmax, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "68590047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining the R² scores for each cluster solution on value based variables\n",
    "r2_scores = {}\n",
    "\n",
    "r2_scores['kmeans'] = get_r2_scores(df_value, value_based_features, kmeans)\n",
    "\n",
    "for linkage in ['complete', 'average', 'single', 'ward']:\n",
    "    r2_scores[linkage] = get_r2_scores(\n",
    "        df_value,                 # data\n",
    "        value_based_features,   # features of perspective\n",
    "        # use HClust, changing the linkage at each iteration\n",
    "        hierarchical.set_params(linkage=linkage) \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f614da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_scores_df = pd.DataFrame(r2_scores)\n",
    "r2_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac0ebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the R² scores for each cluster solution on value based variables\n",
    "r2_scores_df.plot.line(figsize=(10,7))\n",
    "\n",
    "plt.title(\"Value Based Variables:\\nR² plot for various clustering methods\\n\", fontsize=21)\n",
    "plt.legend(title=\"Cluster methods\", title_fontsize=11)\n",
    "plt.xlabel(\"Number of clusters\", fontsize=13)\n",
    "plt.ylabel(\"R² metric\", fontsize=13) \n",
    "plt.show()      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd905ad",
   "metadata": {},
   "source": [
    "## 2. Preference-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "b1e96c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the R² scores for each cluster solution on value Preference based variables\n",
    "r2_scores_prf = {}\n",
    "\n",
    "r2_scores_prf['kmeans'] = get_r2_scores(df_prf,preference_based_features, kmeans)\n",
    "\n",
    "for linkage in ['complete', 'average', 'single', 'ward']:\n",
    "    r2_scores_prf[linkage] = get_r2_scores(\n",
    "        df_prf,                 # data\n",
    "        preference_based_features,   # features of perspective\n",
    "        # use HClust, changing the linkage at each iteration\n",
    "        hierarchical.set_params(linkage=linkage) \n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a3a308",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_scores_df_prf = pd.DataFrame(r2_scores_prf)\n",
    "r2_scores_df_prf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607c4d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the R² scores for each cluster solution on Preference based variables\n",
    "r2_scores_df_prf.plot.line(figsize=(10,7))\n",
    "\n",
    "plt.title(\"Preference Based:\\nR² plot for various clustering methods\\n\", fontsize=21)\n",
    "plt.legend(title=\"Cluster methods\", title_fontsize=11)\n",
    "plt.xlabel(\"Number of clusters\", fontsize=13)\n",
    "plt.ylabel(\"R² metric\", fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3227827",
   "metadata": {},
   "source": [
    "## Merging the Perspectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "1cd3834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the right clustering (algorithm and number of clusters) for each perspective\n",
    "kmeans_value = KMeans(\n",
    "    n_clusters=3,\n",
    "    init='k-means++',\n",
    "    n_init=20,\n",
    "    random_state=42\n",
    ")\n",
    "value_labels = kmeans_value.fit_predict(df_value)\n",
    "\n",
    "kmeans_prf = KMeans(\n",
    "    n_clusters=3,\n",
    "    init='k-means++',\n",
    "    n_init=20,\n",
    "    random_state=42\n",
    ")\n",
    "preference_labels = kmeans_prf.fit_predict(df_prf)\n",
    "\n",
    "df['value_labels'] = value_labels\n",
    "df['preference_labels'] = preference_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4abed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count label frequencies (contigency table)\n",
    "\n",
    "pd.crosstab(df['value_labels'],\n",
    "            df['preference_labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac7a1d6",
   "metadata": {},
   "source": [
    "- clusters with few points:\n",
    "  \n",
    "  * (0,0)\n",
    "  * (1,1)\n",
    "  * (2,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21da2af",
   "metadata": {},
   "source": [
    "### Manual merging: Merge lowest frequency clusters into closest clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c61fd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c392f83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get centroids of clusters based on value, preference, and behavior labels\n",
    "df_centroids = df.groupby(['value_labels', 'preference_labels'])[metric_features].mean()\n",
    "\n",
    "df_centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "00dbebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clusters with low frequency to be merged:\n",
    "# (Value_label, Preferences_label)\n",
    "to_merge = [(0,0), (1,1) ,(2,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e195de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the euclidean distance matrix between the centroids\n",
    "centroid_dists = euclidean = pairwise_distances(df_centroids)\n",
    "\n",
    "df_dists = pd.DataFrame(\n",
    "    centroid_dists, \n",
    "    columns=df_centroids.index, \n",
    "    index=df_centroids.index\n",
    ")\n",
    "\n",
    "df_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a352ee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging each low frequency clustering (source) \n",
    "# to the closest cluster (target)\n",
    "\n",
    "source_target = {}\n",
    "\n",
    "for clus in to_merge:\n",
    "    # If cluster to merge (source) has not yet been used as target\n",
    "    if clus not in source_target.values():\n",
    "        # Add this cluster to source_target map as key\n",
    "        # Use the cluster with the smallest distance to it as value\n",
    "        source_target[clus] = df_dists.loc[clus].sort_values().index[1]\n",
    "\n",
    "source_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "c7ab40ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df.copy()\n",
    "\n",
    "# Changing the Value_labels and Preferences_labels based on source_target\n",
    "for source, target in source_target.items():\n",
    "    mask = (df_['value_labels']==source[0]) & (df_['preference_labels']==source[1])\n",
    "    df_.loc[mask, 'value_labels'] = target[0]\n",
    "    df_.loc[mask, 'preference_labels'] = target[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43011c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New contigency table\n",
    "\n",
    "pd.crosstab(df_['value_labels'],\n",
    "            df_['preference_labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964ea348",
   "metadata": {},
   "source": [
    "### Merging using Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6760e6",
   "metadata": {},
   "source": [
    "Doesnt care about how large our clustering is; just how far from the others are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d03f2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centroids of the concatenated cluster labels\n",
    "df_centroids = df.groupby(['value_labels', 'preference_labels'])\\\n",
    "    [metric_features].mean()\n",
    "df_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "id": "4a08d318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Hierarchical clustering to merge the concatenated cluster centroids\n",
    "linkage = 'ward'\n",
    "hclust = AgglomerativeClustering(\n",
    "    linkage=linkage, \n",
    "    metric='euclidean', \n",
    "    distance_threshold=0, \n",
    "    n_clusters=None\n",
    ")\n",
    "\n",
    "hclust_labels = hclust.fit_predict(df_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cfd675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py\n",
    "\n",
    "# create the counts of samples under each node (number of points being merged)\n",
    "counts = np.zeros(hclust.children_.shape[0])\n",
    "n_samples = len(hclust.labels_)\n",
    "\n",
    "# hclust.children_ contains the observation ids that are being merged together\n",
    "# At the i-th iteration, children[i][0] and children[i][1] are merged to form node n_samples + i\n",
    "for i, merge in enumerate(hclust.children_):\n",
    "    # track the number of observations in the current cluster being formed\n",
    "    current_count = 0\n",
    "    for child_idx in merge:\n",
    "        if child_idx < n_samples:\n",
    "            # If this is True, then we are merging an observation\n",
    "            current_count += 1  # leaf node\n",
    "        else:\n",
    "            # Otherwise, we are merging a previously formed cluster\n",
    "            current_count += counts[child_idx - n_samples]\n",
    "    counts[i] = current_count\n",
    "\n",
    "# the hclust.children_ is used to indicate the two points/clusters being merged (dendrogram's u-joins)\n",
    "# the hclust.distances_ indicates the distance between the two points/clusters (height of the u-joins)\n",
    "# the counts indicate the number of points being merged (dendrogram's x-axis)\n",
    "linkage_matrix = np.column_stack(\n",
    "    [hclust.children_, hclust.distances_, counts]\n",
    ").astype(float)\n",
    "\n",
    "# Plot the corresponding dendrogram\n",
    "sns.set()\n",
    "fig = plt.figure(figsize=(11,5))\n",
    "# The Dendrogram parameters need to be tuned\n",
    "\n",
    "y_threshold = 4.0\n",
    "\n",
    "dendrogram(linkage_matrix, \n",
    "           truncate_mode='level', \n",
    "           labels=df_centroids.index, p=5, \n",
    "           color_threshold=y_threshold, \n",
    "           above_threshold_color='k')\n",
    "\n",
    "plt.hlines(y_threshold, 0, 1000, colors=\"r\", linestyles=\"dashed\")\n",
    "plt.title(f'Hierarchical Clustering - {linkage.title()}\\'s Dendrogram', fontsize=21)\n",
    "plt.xlabel('Number of points in node (or index of point if no parenthesis)')\n",
    "plt.ylabel(f'Euclidean Distance', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6096c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-running the Hierarchical clustering based on the correct number of clusters\n",
    "hclust = AgglomerativeClustering(\n",
    "    linkage='ward', \n",
    "    metric='euclidean', \n",
    "    n_clusters=5\n",
    ")\n",
    "hclust_labels = hclust.fit_predict(df_centroids)\n",
    "df_centroids['hclust_labels'] = hclust_labels\n",
    "\n",
    "df_centroids  # centroid's cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097a0410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapper between concatenated clusters and hierarchical clusters\n",
    "cluster_mapper = df_centroids['hclust_labels'].to_dict()\n",
    "cluster_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85cf22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df.copy()\n",
    "\n",
    "# Mapping the hierarchical clusters on the centroids to the observations\n",
    "df_['merged_labels'] = df_.apply(\n",
    "    lambda row: cluster_mapper[\n",
    "        (row['value_labels'], row['preference_labels'])\n",
    "    ], axis=1\n",
    ")\n",
    "\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44780e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged cluster centroids\n",
    "df_.groupby('merged_labels').mean(numeric_only=True)[metric_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0da34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge cluster contigency table\n",
    "# Getting size of each final cluster\n",
    "df_counts = df_.groupby('merged_labels')\\\n",
    "    .size()\\\n",
    "    .to_frame()\n",
    "\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5435293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the Value and Preference labels\n",
    "df_counts = df_counts\\\n",
    "    .rename({v:k for k, v in cluster_mapper.items()})\\\n",
    "    .reset_index()\n",
    "\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4271c6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts['value_labels'] = df_counts['merged_labels'].apply(lambda x: x[0])\n",
    "df_counts['preference_labels'] = df_counts['merged_labels'].apply(lambda x: x[1])\n",
    "\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc014a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts.pivot(values=0, index='value_labels', columns='preference_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "971a9bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e47c9af",
   "metadata": {},
   "source": [
    "# Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "id": "329d6d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_profiles(df, label_columns, figsize, \n",
    "                     cmap=\"tab10\",\n",
    "                     compare_titles=None):\n",
    "    \"\"\"\n",
    "    Pass df with labels columns of one or multiple clustering labels. \n",
    "    Then specify this label columns to perform the cluster profile according to them.\n",
    "    \"\"\"\n",
    "    \n",
    "    if compare_titles == None:\n",
    "        compare_titles = [\"\"]*len(label_columns)\n",
    "        \n",
    "    fig, axes = plt.subplots(nrows=len(label_columns), \n",
    "                             ncols=2, \n",
    "                             figsize=figsize, \n",
    "                             constrained_layout=True,\n",
    "                             squeeze=False)\n",
    "    for ax, label, titl in zip(axes, label_columns, compare_titles):\n",
    "        # Filtering df\n",
    "        drop_cols = [i for i in label_columns if i!=label]\n",
    "        dfax = df.drop(drop_cols, axis=1)\n",
    "        \n",
    "        # Getting the cluster centroids and counts\n",
    "        centroids = dfax.groupby(by=label, as_index=False).mean()\n",
    "        counts = dfax.groupby(by=label, as_index=False).count().iloc[:,[0,1]]\n",
    "        counts.columns = [label, \"counts\"]\n",
    "        \n",
    "        # Setting Data\n",
    "        pd.plotting.parallel_coordinates(centroids, \n",
    "                                            label, \n",
    "                                            color = sns.color_palette(cmap),\n",
    "                                            ax=ax[0])\n",
    "\n",
    "\n",
    "\n",
    "        sns.barplot(x=label, \n",
    "                    hue=label,\n",
    "                    y=\"counts\", \n",
    "                    data=counts, \n",
    "                    ax=ax[1], \n",
    "                    palette=sns.color_palette(cmap),\n",
    "                    legend=False\n",
    "                    )\n",
    "\n",
    "        #Setting Layout\n",
    "        handles, _ = ax[0].get_legend_handles_labels()\n",
    "        cluster_labels = [\"Cluster {}\".format(i) for i in range(len(handles))]\n",
    "        ax[0].annotate(text=titl, xy=(0.95,1.1), xycoords='axes fraction', fontsize=13, fontweight = 'heavy') \n",
    "        ax[0].axhline(color=\"black\", linestyle=\"--\")\n",
    "        ax[0].set_title(\"Cluster Means - {} Clusters\".format(len(handles)), fontsize=13)\n",
    "        ax[0].set_xticklabels(ax[0].get_xticklabels(), \n",
    "                              rotation=40,\n",
    "                              ha='right'\n",
    "                              )\n",
    "        \n",
    "        ax[0].legend(handles, cluster_labels,\n",
    "                     loc='center left', bbox_to_anchor=(1, 0.5), title=label\n",
    "                     ) # Adaptable to number of clusters\n",
    "        \n",
    "        ax[1].set_xticks([i for i in range(len(handles))])\n",
    "        ax[1].set_xticklabels(cluster_labels)\n",
    "        ax[1].set_xlabel(\"\")\n",
    "        ax[1].set_ylabel(\"Absolute Frequency\")\n",
    "        ax[1].set_title(\"Cluster Sizes - {} Clusters\".format(len(handles)), fontsize=13)\n",
    "        \n",
    "        \n",
    "    \n",
    "    # plt.subplots_adjust(hspace=0.4, top=0.90)\n",
    "    plt.suptitle(\"Cluster Simple Profiling\", fontsize=23)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03eb74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profilling each cluster \n",
    "cluster_profiles(\n",
    "    df = df[metric_features + ['value_labels', 'preference_labels', 'merged_labels']], \n",
    "    label_columns = ['value_labels', 'preference_labels', 'merged_labels'], \n",
    "    figsize = (28, 13), \n",
    "    compare_titles = [\"Value clustering\", \"Preference clustering\", \"Merged clusters\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f19b3b",
   "metadata": {},
   "source": [
    "## Profiling with unused / categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c7d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_metric_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd511494",
   "metadata": {},
   "source": [
    "## City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258cbda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city = df[['merged_labels',\n",
    "            'enc_customer_city_2',\n",
    "            'enc_customer_city_4',\n",
    "            'enc_customer_city_8']].groupby(['merged_labels']).sum()\n",
    "\n",
    "df_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ca8b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['merged_labels']].groupby(['merged_labels']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e037e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, \n",
    "                         df['merged_labels'].nunique() + 1, # Add an extra ax for population countplot\n",
    "                         figsize=(16,4),\n",
    "                         tight_layout=True,\n",
    "                        #  sharey=True,\n",
    "                         )\n",
    "\n",
    "\n",
    "for i in range(len(axes.flatten())): \n",
    "    ax = axes[i]\n",
    "    if i == 0:\n",
    "        sns.countplot(df, \n",
    "                        x='customer_city', \n",
    "                        order = df['customer_city'].value_counts().index,\n",
    "                        ax=ax)\n",
    "        ax.set_title(\"All Data\")\n",
    "        \n",
    "    else:    \n",
    "        sns.countplot(df.loc[df['merged_labels']==i-1], \n",
    "                    x='customer_city', \n",
    "                    order = df['customer_city'].value_counts().index,\n",
    "                    ax=ax)\n",
    "        ax.set_title(\"Cluster {}\".format(i-1))\n",
    "    \n",
    "    ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "plt.suptitle(\"City Counts\", )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6ffc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl_city = df.groupby([\n",
    "    \"merged_labels\", \n",
    "    \"customer_city\",\n",
    "    ])['customer_city'].size().unstack()\n",
    "\n",
    "df_cl_city\n",
    "\n",
    "\n",
    "df_cl_city.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d725edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl_city_pct = df_cl_city.copy()\n",
    "for i in df['customer_city'].unique():\n",
    "    df_cl_city_pct[i] = 100*df_cl_city_pct[i]/df['merged_labels'].value_counts().sort_index().values\n",
    "\n",
    "df_cl_city_pct.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8dec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, df['merged_labels'].nunique(), \n",
    "                         figsize=(12,4),\n",
    "                         sharey=True,)\n",
    "\n",
    "for ax, clust in zip(axes.flatten(), range(df['merged_labels'].nunique())): \n",
    "    df_cl = df.loc[df['merged_labels']==clust]\n",
    "    sns.countplot(df_cl, \n",
    "                  x='customer_city', \n",
    "                  order = df['customer_city'].value_counts().index,\n",
    "                  ax=ax)\n",
    "    \n",
    "    ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "    ax.set_xlabel(\"\")\n",
    "\n",
    "    ax.set_title(\"Cluster {}\".format(clust))\n",
    "plt.suptitle(\"Count of City by Cluster\", y=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6255ffa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df[\"merged_labels\"],df[\"customer_city\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917d8eaa",
   "metadata": {},
   "source": [
    "## Promo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14071bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4881c09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_promo = df[['merged_labels',\n",
    "            'enc_last_promo',\n",
    "            ]].groupby(['merged_labels']).sum()\n",
    "\n",
    "df_promo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8373e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['merged_labels']].groupby(['merged_labels']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1326b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, \n",
    "                         df['merged_labels'].nunique() + 1, # Add an extra ax for population countplot\n",
    "                         figsize=(16,4),\n",
    "                         tight_layout=True,\n",
    "                        #  sharey=True,\n",
    "                         )\n",
    "\n",
    "\n",
    "for i in range(len(axes.flatten())): \n",
    "    ax = axes[i]\n",
    "    if i == 0:\n",
    "        sns.countplot(df, \n",
    "                        x='last_promo', \n",
    "                        order = df['last_promo'].value_counts().index,\n",
    "                        ax=ax)\n",
    "        ax.set_title(\"All Data\")\n",
    "        \n",
    "    else:    \n",
    "        sns.countplot(df.loc[df['merged_labels']==i-1], \n",
    "                    x='last_promo', \n",
    "                    order = df['last_promo'].value_counts().index,\n",
    "                    ax=ax)\n",
    "        ax.set_title(\"Cluster {}\".format(i-1))\n",
    "    \n",
    "    ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "plt.suptitle(\"Promo Counts\", )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a0c5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl_promo = df.groupby([\n",
    "    \"merged_labels\", \n",
    "    \"last_promo\",\n",
    "    ])['last_promo'].size().unstack()\n",
    "\n",
    "df_cl_promo\n",
    "\n",
    "\n",
    "df_cl_promo.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8488f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl_promo_pct = df_cl_promo.copy()\n",
    "for i in df['last_promo'].unique():\n",
    "    df_cl_promo_pct[i] = 100*df_cl_promo_pct[i]/df['merged_labels'].value_counts().sort_index().values\n",
    "\n",
    "df_cl_promo_pct.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619a0639",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, df['merged_labels'].nunique(), \n",
    "                         figsize=(12,4),\n",
    "                         sharey=True,)\n",
    "\n",
    "for ax, clust in zip(axes.flatten(), range(df['merged_labels'].nunique())): \n",
    "    df_cl = df.loc[df['merged_labels']==clust]\n",
    "    sns.countplot(df_cl, \n",
    "                  x='last_promo', \n",
    "                  order = df['last_promo'].value_counts().index,\n",
    "                  ax=ax)\n",
    "    \n",
    "    ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "    ax.set_xlabel(\"\")\n",
    "\n",
    "    ax.set_title(\"Cluster {}\".format(clust))\n",
    "plt.suptitle(\"Count of Last Promo by Cluster\", y=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e11d42",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b159a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enc = df[['merged_labels',\n",
    "            'enc_is_chain_0',\n",
    "            'enc_is_chain_1',\n",
    "]].groupby(['merged_labels']).sum()\n",
    "\n",
    "df_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5872ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['merged_labels']].groupby(['merged_labels']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c85343",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, \n",
    "                         df['merged_labels'].nunique() + 1, # Add an extra ax for population countplot\n",
    "                         figsize=(16,4),\n",
    "                         tight_layout=True,\n",
    "                        #  sharey=True,\n",
    "                         )\n",
    "\n",
    "\n",
    "for i in range(len(axes.flatten())): \n",
    "    ax = axes[i]\n",
    "    if i == 0:\n",
    "        sns.countplot(df, \n",
    "                        x='is_chain', \n",
    "                        order = df['is_chain'].value_counts().index,\n",
    "                        ax=ax)\n",
    "        ax.set_title(\"All Data\")\n",
    "        \n",
    "    else:    \n",
    "        sns.countplot(df.loc[df['is_chain']==i-1], \n",
    "                    x='is_chain', \n",
    "                    order = df['is_chain'].value_counts().index,\n",
    "                    ax=ax)\n",
    "        ax.set_title(\"Cluster {}\".format(i-1))\n",
    "    \n",
    "    ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "plt.suptitle(\"Chain Counts\", )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77faf292",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl_chain = df.groupby([\n",
    "    \"merged_labels\", \n",
    "    \"is_chain\",\n",
    "    ])['is_chain'].size().unstack()\n",
    "\n",
    "df_cl_chain\n",
    "\n",
    "\n",
    "df_cl_chain.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f4e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl_chain_pct = df_cl_chain.copy()\n",
    "for i in df['is_chain'].unique():\n",
    "    df_cl_chain_pct[i] = 100*df_cl_chain_pct[i]/df['merged_labels'].value_counts().sort_index().values\n",
    "\n",
    "df_cl_chain_pct.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538c334d",
   "metadata": {},
   "source": [
    "## Payment Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d393aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pay = df[['merged_labels',\n",
    "            'enc_payment_method_CARD',\n",
    "       'enc_payment_method_CASH', 'enc_payment_method_DIGI',\n",
    "]].groupby(['merged_labels']).sum()\n",
    "\n",
    "df_pay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023cfda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['merged_labels']].groupby(['merged_labels']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58824a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, \n",
    "                         df['merged_labels'].nunique() + 1, # Add an extra ax for population countplot\n",
    "                         figsize=(16,4),\n",
    "                         tight_layout=True,\n",
    "                        #  sharey=True,\n",
    "                         )\n",
    "\n",
    "\n",
    "for i in range(len(axes.flatten())): \n",
    "    ax = axes[i]\n",
    "    if i == 0:\n",
    "        sns.countplot(df, \n",
    "                        x='payment_method', \n",
    "                        order = df['payment_method'].value_counts().index,\n",
    "                        ax=ax)\n",
    "        ax.set_title(\"All Data\")\n",
    "        \n",
    "    else:    \n",
    "        sns.countplot(df.loc[df['payment_method']==i-1], \n",
    "                    x='payment_method', \n",
    "                    order = df['payment_method'].value_counts().index,\n",
    "                    ax=ax)\n",
    "        ax.set_title(\"Cluster {}\".format(i-1))\n",
    "    \n",
    "    ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "plt.suptitle(\"Payment Counts\", )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2c2910",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl_pay = df.groupby([\n",
    "    \"merged_labels\", \n",
    "    \"payment_method\",\n",
    "    ])['payment_method'].size().unstack()\n",
    "\n",
    "df_cl_pay\n",
    "\n",
    "\n",
    "df_cl_pay.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3426b787",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl_pay_pct = df_cl_pay.copy()\n",
    "for i in df['payment_method'].unique():\n",
    "    df_cl_pay_pct[i] = 100*df_cl_pay_pct[i]/df['merged_labels'].value_counts().sort_index().values\n",
    "\n",
    "df_cl_pay_pct.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b0d5ad",
   "metadata": {},
   "source": [
    "## Age Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4428d608",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age = df[['merged_labels',\n",
    "            'enc_age_group',\n",
    "       \n",
    "]].groupby(['merged_labels']).sum()\n",
    "\n",
    "df_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d78343",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['merged_labels']].groupby(['merged_labels']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1579b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, \n",
    "                         df['merged_labels'].nunique() + 1, # Add an extra ax for population countplot\n",
    "                         figsize=(16,4),\n",
    "                         tight_layout=True,\n",
    "                        #  sharey=True,\n",
    "                         )\n",
    "\n",
    "\n",
    "for i in range(len(axes.flatten())): \n",
    "    ax = axes[i]\n",
    "    if i == 0:\n",
    "        sns.countplot(df, \n",
    "                        x='age_group', \n",
    "                        order = df['age_group'].value_counts().index,\n",
    "                        ax=ax)\n",
    "        ax.set_title(\"All Data\")\n",
    "        \n",
    "    else:    \n",
    "        sns.countplot(df.loc[df['age_group']==i-1], \n",
    "                    x='age_group', \n",
    "                    order = df['age_group'].value_counts().index,\n",
    "                    ax=ax)\n",
    "        ax.set_title(\"Cluster {}\".format(i-1))\n",
    "    \n",
    "    ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "plt.suptitle(\"Age Group Counts\", )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac43d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl_age = df.groupby([\n",
    "    \"merged_labels\", \n",
    "    \"age_group\",\n",
    "    ])['age_group'].size().unstack()\n",
    "\n",
    "df_cl_age\n",
    "\n",
    "\n",
    "df_cl_age.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7eec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl_age_pct = df_cl_age.copy()\n",
    "for i in df['age_group'].unique():\n",
    "    df_cl_age_pct[i] = 100*df_cl_age_pct[i]/df['merged_labels'].value_counts().sort_index().values\n",
    "\n",
    "df_cl_age_pct.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0c409f",
   "metadata": {},
   "source": [
    "## Cuisine Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170d3c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponha que 'numerical_var' seja a variável numérica e 'cluster_label' os rótulos dos clusters\n",
    "cluster_profile = df.groupby('merged_labels')['cuisine_diversity'].agg(['mean', 'std', 'min', 'max', 'median'])\n",
    "\n",
    "# Visualizar o perfil dos clusters\n",
    "print(cluster_profile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d264d312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular médias\n",
    "cluster_means = df.groupby('merged_labels')['cuisine_diversity'].mean()\n",
    "\n",
    "# Plotar\n",
    "cluster_means.plot(kind='bar', color='steelblue', figsize=(8, 5), edgecolor='black')\n",
    "plt.title('Cuisine Diversity by Cluster')\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('Mean of cuisine diveristy')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b30c83e",
   "metadata": {},
   "source": [
    "# ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "7ba96434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ss_variables(df):\n",
    "    \"\"\"Get the SS for each variable\n",
    "    \"\"\"\n",
    "    ss_vars = df.var() * (df.count() - 1)\n",
    "    return ss_vars\n",
    "\n",
    "def r2_variables(df, labels):\n",
    "    \"\"\"Get the R² for each variable\n",
    "    \"\"\"\n",
    "    sst_vars = get_ss_variables(df)\n",
    "    ssw_vars = np.sum(df.groupby(labels).apply(get_ss_variables))\n",
    "    return 1 - ssw_vars/sst_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27f073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are essentially decomposing the R² into the R² for each variable\n",
    "r2_variables(df[metric_features + ['merged_labels']], 'merged_labels').drop('merged_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea86165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "X = df[metric_features]\n",
    "y = df.merged_labels\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Fitting the decision tree\n",
    "dt = DecisionTreeClassifier(random_state=42, max_depth=3)\n",
    "dt.fit(X_train, y_train)\n",
    "print(\"It is estimated that in average, we are able to predict {0:.2f}% of the customers correctly\".format(dt.score(X_test, y_test)*100))\n",
    "\n",
    "\n",
    "# Performance on training data\n",
    "train_score = dt.score(X_train, y_train)\n",
    "# Performance on test data\n",
    "test_score = dt.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_score * 100:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_score * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307debb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing feature importance\n",
    "pd.Series(dt.feature_importances_, index=X_train.columns).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367be0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the cluster labels of the outliers\n",
    "df_out['merged_labels'] = dt.predict(df_out[metric_features])\n",
    "df_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "id": "619367eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate df_out with the predicted 'merged_labels' back to the original df\n",
    "df = pd.concat([df, df_out[['merged_labels']]], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015a280f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
